{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c1231-83de-45bc-a8c7-ff64aa6495b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24040b56-a634-436f-a541-f6b2d56591bd",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "Simple linear regression is a statistical method used to model the relationship between two variables, where one is considered the independent variable (predictor) and the other is the dependent variable (the outcome you want to predict). The relationship is modeled as a straight line equation:\n",
    "\n",
    "Y = b0 + b1 * X\n",
    "\n",
    "Y represents the dependent variable.\n",
    "X represents the independent variable.\n",
    "b0 is the intercept, the value of Y when X is 0.\n",
    "b1 is the slope, indicating how much Y changes for a unit change in X.\n",
    "Example of Simple Linear Regression:\n",
    "Let's say we want to predict a person's weight (Y) based on their height (X). We collect data from several individuals and use simple linear regression to establish the relationship between height and weight. The equation might look like:\n",
    "\n",
    "Weight = b0 + b1 * Height\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression is an extension of simple linear regression where we consider more than one independent variable to predict the dependent variable. It assumes a linear relationship between the dependent variable and multiple predictors:\n",
    "\n",
    "Y = b0 + b1 * X1 + b2 * X2 + ... + bn * Xn\n",
    "\n",
    "Y represents the dependent variable.\n",
    "X1, X2, ..., Xn represent the independent variables.\n",
    "b0 is the intercept, the value of Y when all X variables are 0.\n",
    "b1, b2, ..., bn are the slopes, indicating how much Y changes for a unit change in the corresponding X variables.\n",
    "Example of Multiple Linear Regression:\n",
    "Suppose you want to predict a house's sale price (Y) based on various features such as the number of bedrooms (X1), the square footage (X2), and the neighborhood's crime rate (X3). Multiple linear regression allows you to create a model like this:\n",
    "\n",
    "Sale Price = b0 + b1 * Bedrooms + b2 * Square Footage + b3 * Crime Rate\n",
    "\n",
    "In this case, you are considering multiple factors to predict the sale price, not just one as in simple linear regression.\n",
    "\n",
    "In summary, the main difference is the number of independent variables involved. Simple linear regression deals with one predictor, while multiple linear regression deals with two or more predictors to make more complex predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45245bb7-b212-4c3f-bb36-5a0023b550a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58599b2-fdd3-47ee-b63b-54b070cb2a9e",
   "metadata": {},
   "source": [
    "Linear regression relies on several key assumptions to be valid. Violation of these assumptions can affect the accuracy and reliability of the regression model. Here are the primary assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be approximately linear. You can check this assumption by creating scatter plots of the variables and visually assessing whether they exhibit a linear pattern. You can also use residual plots to detect non-linearity.\n",
    "\n",
    "Independence of Errors: The errors (residuals) should be independent of each other. In other words, the value of the error for one data point should not depend on the value of the error for another data point. This assumption is often assumed to be met if the data points are collected independently. You can also check for autocorrelation in residuals using statistical tests or plots.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables (homoscedasticity) rather than increasing or decreasing with the values of the independent variable (heteroscedasticity). You can check for heteroscedasticity by plotting the residuals against the predicted values or the independent variables. If you see a funnel-like pattern, it may indicate heteroscedasticity.\n",
    "\n",
    "Normality of Residuals: The residuals should be normally distributed. You can assess this assumption by creating a histogram or a Q-Q plot of the residuals and checking if they resemble a normal distribution. Alternatively, you can use formal statistical tests such as the Shapiro-Wilk test or the Anderson-Darling test to check for normality.\n",
    "\n",
    "No or Little Multicollinearity: Multicollinearity occurs when independent variables are highly correlated with each other. It can make it challenging to separate the individual effects of each predictor on the dependent variable. You can use correlation matrices or variance inflation factor (VIF) values to check for multicollinearity.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can perform the following:\n",
    "\n",
    "Visual Inspection: Create scatter plots for each independent variable against the dependent variable to check linearity and for residual plots to assess homoscedasticity.\n",
    "\n",
    "Statistical Tests: Utilize statistical tests like the Shapiro-Wilk test for normality of residuals or tests for autocorrelation to check independence of errors.\n",
    "\n",
    "Residual Analysis: Analyze the residuals to check for patterns or deviations from the assumptions. You can also use normal probability plots to assess the normality of residuals.\n",
    "\n",
    "Diagnostic Plots: Create diagnostic plots, such as a Q-Q plot or a scatterplot of residuals versus fitted values, to visually assess the validity of assumptions.\n",
    "\n",
    "Numerical Metrics: Compute VIF values to quantify multicollinearity and assess whether they are above a certain threshold (commonly VIF > 5 is considered indicative of multicollinearity).\n",
    "\n",
    "If the assumptions are significantly violated, you may need to consider data transformations, removing outliers, adding additional features, or using a different modeling approach (e.g., non-linear regression methods) to address the issues and build a more accurate model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13992-8cbc-411c-8e86-84f0902caeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4013be7-ef8c-4ef9-aa2b-aa9261a49f5a",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "Intercept (b0): The intercept represents the estimated value of the dependent variable when all independent variables are set to zero. It is the point at which the regression line crosses the vertical axis. In many cases, the intercept may not have a meaningful interpretation, especially if setting all predictors to zero doesn't make sense in the context of your data.\n",
    "\n",
    "Slope (b1, b2, etc.): The slope(s) represent the change in the dependent variable (Y) for a one-unit change in the corresponding independent variable (X1, X2, etc.), while holding all other variables constant. It indicates the strength and direction of the relationship between the independent variable and the dependent variable.\n",
    "\n",
    "Here's an example using a real-world scenario:\n",
    "\n",
    "Scenario: Predicting Employee Salary\n",
    "\n",
    "Suppose you are an HR manager, and you want to predict an employee's salary (Y) based on the number of years of experience (X1) and the level of education (X2, where 0 represents high school, 1 represents a bachelor's degree, and 2 represents a master's degree).\n",
    "\n",
    "Your multiple linear regression model looks like this:\n",
    "\n",
    "Salary = b0 + b1 * Years of Experience + b2 * Level of Education\n",
    "\n",
    "Intercept (b0): In this context, the intercept represents the estimated salary when an employee has no years of experience (0) and a high school level of education (X2 = 0). It is the base salary for individuals with no experience and only a high school diploma.\n",
    "\n",
    "Slope for Years of Experience (b1): The slope for years of experience tells you how much an employee's salary is expected to increase for each additional year of experience, assuming the level of education remains constant. For example, if b1 is $5,000, it means that, on average, an employee's salary increases by $5,000 for each additional year of experience.\n",
    "\n",
    "Slope for Level of Education (b2): The slope for the level of education indicates the difference in salary based on the education level, while holding years of experience constant. If b2 is $10,000, it means that, on average, employees with a bachelor's degree earn $10,000 more than those with only a high school diploma, while having the same years of experience.\n",
    "\n",
    "So, in this scenario, the intercept gives you the starting point for those with no experience and a high school diploma, while the slopes provide the rate of change in salary associated with years of experience and the difference in salary due to different levels of education. These interpretations are valuable for understanding the relationship between the predictors and the dependent variable in your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8535fcb-5d55-4061-914c-933767b471ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a8166-fd6b-4862-9fd1-71b1740cd27a",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used in machine learning to minimize a cost function or loss function. It is a fundamental technique for training machine learning models, especially in tasks like linear regression, logistic regression, neural networks, and other models that involve parameter tuning.\n",
    "\n",
    "Here's an overview of the concept of gradient descent and how it is used in machine learning:\n",
    "\n",
    "Objective of Gradient Descent:\n",
    "\n",
    "In machine learning, the goal is often to find the set of model parameters (weights) that minimize a cost function (also known as a loss function). This cost function measures the error or the difference between the predicted values and the actual target values.\n",
    "The objective is to find the parameter values that result in the lowest possible cost or error.\n",
    "Gradient Descent Process:\n",
    "\n",
    "Gradient descent is an iterative optimization process that starts with an initial guess for the parameter values.\n",
    "It calculates the gradient (partial derivatives) of the cost function with respect to each parameter. The gradient provides the direction and magnitude of the steepest increase in the cost function.\n",
    "Gradient descent then updates the parameter values by moving them in the opposite direction of the gradient. The step size is controlled by a parameter called the learning rate.\n",
    "The process is repeated for a specified number of iterations or until convergence (when the change in the cost function becomes very small).\n",
    "Learning Rate:\n",
    "\n",
    "The learning rate is a hyperparameter that determines the step size at each iteration. It controls how far the algorithm adjusts the parameters in each step. Choosing the right learning rate is crucial because too small a learning rate can lead to slow convergence, while too large a learning rate can cause the algorithm to overshoot the minimum.\n",
    "Batch, Mini-Batch, and Stochastic Gradient Descent:\n",
    "\n",
    "Gradient descent can be applied in different ways depending on the amount of data used for each parameter update.\n",
    "Batch Gradient Descent: Uses the entire training dataset to calculate the gradient at each step.\n",
    "Mini-Batch Gradient Descent: Divides the dataset into smaller batches and calculates the gradient for each batch.\n",
    "Stochastic Gradient Descent (SGD): Computes the gradient for a single data point at each step.\n",
    "Mini-batch and stochastic versions are often preferred for large datasets because they are computationally more efficient.\n",
    "Convergence and Termination:\n",
    "\n",
    "Gradient descent iterates until it converges, meaning that the change in the cost function becomes very small, or until it reaches a specified number of iterations.\n",
    "Early stopping is a technique used to stop training when the model starts to overfit the data.\n",
    "Applications:\n",
    "\n",
    "Gradient descent is used in various machine learning algorithms, including linear regression, logistic regression, neural network training, support vector machines, and many more.\n",
    "It is a foundational tool for parameter optimization and model training, helping models learn from data and make accurate predictions.\n",
    "In summary, gradient descent is a key algorithm in machine learning used for optimizing model parameters to minimize the cost function. It is a fundamental technique for training models and making them fit the data as accurately as possible.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7ed83-593a-4566-a1f0-09779cc3177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf70f25-767f-41ca-bdc0-a51c4d9a55d6",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows you to model the relationship between a dependent variable (Y) and multiple independent variables (X1, X2, X3, etc.). While simple linear regression deals with a single independent variable, multiple linear regression can handle two or more independent variables simultaneously. The multiple linear regression model can be expressed as:\n",
    "\n",
    "Y = b0 + b1 * X1 + b2 * X2 + b3 * X3 + ... + bn * Xn + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "Y represents the dependent variable (the one you want to predict).\n",
    "X1, X2, X3, ..., Xn are the independent variables or predictors.\n",
    "b0 is the intercept, representing the predicted value of Y when all X variables are set to zero.\n",
    "b1, b2, b3, ..., bn are the coefficients or slopes associated with each independent variable, indicating how much Y is expected to change for a one-unit change in the corresponding X variable.\n",
    "ε represents the error term, which accounts for the unexplained variation in Y.\n",
    "Differences between Multiple Linear Regression and Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "Simple Linear Regression: It involves only one independent variable (X).\n",
    "Multiple Linear Regression: It includes two or more independent variables (X1, X2, X3, etc.).\n",
    "Model Equation:\n",
    "\n",
    "Simple Linear Regression: Y = b0 + b1 * X\n",
    "Multiple Linear Regression: Y = b0 + b1 * X1 + b2 * X2 + b3 * X3 + ... + bn * Xn + ε\n",
    "Interpretation:\n",
    "\n",
    "In simple linear regression, you interpret the slope (b1) as the change in Y for a one-unit change in X.\n",
    "In multiple linear regression, you interpret each slope (b1, b2, b3, etc.) as the change in Y for a one-unit change in the corresponding X variable, while holding all other X variables constant.\n",
    "Complexity:\n",
    "\n",
    "Multiple linear regression is a more complex model as it considers the combined effects of multiple independent variables on the dependent variable.\n",
    "Simple linear regression is simpler, dealing with a single predictor's impact on the dependent variable.\n",
    "Use Cases:\n",
    "\n",
    "Simple Linear Regression: Appropriate when you want to examine the relationship between two variables or when you believe only one predictor is influencing the dependent variable significantly.\n",
    "Multiple Linear Regression: Useful when multiple predictors are likely to affect the dependent variable or when you want to account for the combined effects of several factors.\n",
    "Multiple linear regression provides a more comprehensive way to model and understand the relationships between multiple independent variables and a dependent variable, allowing for more realistic and complex modeling scenarios in various fields, including economics, social sciences, and natural sciences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec591392-b454-437d-97fe-bfa857c00143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07de58-2037-4635-8350-6539b328c554",
   "metadata": {},
   "source": [
    "Multicollinearity is a common issue in multiple linear regression when two or more independent variables in a model are highly correlated with each other. This high correlation can cause problems in the interpretation of the regression model and can make it difficult to isolate the individual effects of each predictor on the dependent variable. Here's an explanation of multicollinearity and how to detect and address this issue:\n",
    "\n",
    "Concept of Multicollinearity:\n",
    "\n",
    "Multicollinearity occurs when there is a strong linear relationship between two or more independent variables in a multiple linear regression model.\n",
    "It makes it challenging to determine the independent contribution of each predictor because their effects are confounded by the presence of other correlated predictors.\n",
    "Multicollinearity can lead to unstable coefficient estimates, large standard errors, and potentially misleading interpretations of the model.\n",
    "Detection of Multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. High absolute correlation values (close to +1 or -1) suggest potential multicollinearity.\n",
    "Variance Inflation Factor (VIF): Compute the VIF for each predictor. The VIF quantifies how much the variance of the estimated coefficients is inflated due to multicollinearity. A VIF greater than 1 indicates multicollinearity, with larger values indicating a more severe issue.\n",
    "Tolerance: Tolerance is the reciprocal of the VIF. A tolerance value close to 1 suggests low multicollinearity, while values close to 0 indicate high multicollinearity.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove or Combine Variables: If two or more variables are highly correlated, consider removing one or combining them into a single variable that captures their shared information.\n",
    "Feature Selection: Use feature selection techniques to choose a subset of predictors that are the most relevant and less correlated.\n",
    "Data Collection: Gather more data if possible to reduce the impact of multicollinearity.\n",
    "Regularization Techniques: Techniques like Ridge Regression and Lasso Regression can help mitigate multicollinearity by adding penalty terms to the regression equation, encouraging the model to shrink the coefficients towards zero.\n",
    "Centering and Scaling: Standardize or center the variables to give them a mean of zero and standard deviation of one. This can sometimes help reduce multicollinearity.\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can be used to transform the correlated variables into a set of orthogonal (uncorrelated) variables.\n",
    "Domain Knowledge: Use subject matter expertise to decide which variables to retain, especially if their high correlation is expected and meaningful in the context of the problem.\n",
    "It's essential to address multicollinearity because it can affect the stability and interpretability of a multiple linear regression model. By detecting and mitigating multicollinearity, you can improve the accuracy and reliability of your regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759d180-ab01-4cc2-b601-4a8176b8cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1eaf8-3076-4d65-9af5-a8ffcd1e3062",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis used to model the relationship between the dependent variable (Y) and one or more independent variables (X) as an nth-degree polynomial. It's an extension of simple linear regression, which is used when a linear relationship doesn't adequately represent the data. In simple linear regression, the relationship between Y and X is assumed to be a straight line, while in polynomial regression, it allows for more flexible and curved relationships.\n",
    "\n",
    "Here's a description of the polynomial regression model and how it differs from linear regression:\n",
    "\n",
    "Polynomial Regression Model:\n",
    "In polynomial regression, the relationship between Y and X is represented as a polynomial equation of the form:\n",
    "\n",
    "Y = b0 + b1 * X + b2 * X^2 + b3 * X^3 + ... + bn * X^n + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "Y represents the dependent variable.\n",
    "X represents the independent variable(s).\n",
    "b0, b1, b2, ..., bn are the coefficients that are estimated during model training.\n",
    "X^2, X^3, ..., X^n are the independent variable raised to different powers (squared, cubed, etc.).\n",
    "ε represents the error term, accounting for unexplained variation in Y.\n",
    "Differences between Polynomial Regression and Linear Regression:\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "Linear Regression: Assumes a linear relationship between the dependent variable and the independent variable(s), represented as a straight line.\n",
    "Polynomial Regression: Allows for a more complex, curved relationship by introducing terms raised to higher powers (e.g., X^2, X^3, etc.).\n",
    "Model Flexibility:\n",
    "\n",
    "Linear Regression: Simple and inflexible, suitable for modeling linear relationships only.\n",
    "Polynomial Regression: More flexible and capable of capturing non-linear patterns in the data.\n",
    "Number of Features:\n",
    "\n",
    "Linear Regression: Involves a single feature (X) for simple linear regression or multiple features for multiple linear regression.\n",
    "Polynomial Regression: Involves multiple features, with each feature representing X raised to different powers. This can significantly increase the number of features used in the model.\n",
    "Underfitting and Overfitting:\n",
    "\n",
    "Linear Regression: Prone to underfitting when the relationship is not linear or too simplistic for the data.\n",
    "Polynomial Regression: Can capture complex relationships but is also susceptible to overfitting, especially with high-degree polynomials.\n",
    "Interpretability:\n",
    "\n",
    "Linear Regression: Coefficients represent the change in the dependent variable for a one-unit change in the independent variable.\n",
    "Polynomial Regression: Interpretability becomes more complex as the number of polynomial terms increases.\n",
    "Data Transformation:\n",
    "\n",
    "Linear Regression: Typically no transformation of the independent variable is needed.\n",
    "Polynomial Regression: The independent variable may need to be transformed by raising it to different powers to capture the non-linear relationship.\n",
    "In summary, polynomial regression allows for more flexible modeling of non-linear relationships between the dependent variable and the independent variable(s) by introducing polynomial terms. While it provides a powerful tool for capturing complex patterns in the data, it also requires careful tuning to prevent overfitting. Linear regression, on the other hand, assumes a simple linear relationship between variables and is suitable for modeling linear patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979f9ee-3ede-4d13-a155-c6cf3a477875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734640d-aad4-4a92-bc82-2b2e0f35e7b0",
   "metadata": {},
   "source": [
    "Polynomial regression offers advantages and disadvantages compared to linear regression, and the choice between the two depends on the nature of the data and the underlying relationship between variables. Here's a summary of the pros and cons of polynomial regression and when it's preferred:\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture non-linear relationships in the data, allowing for more complex modeling of the relationship between the independent and dependent variables.\n",
    "\n",
    "Better Fit: When the relationship between the variables is curvilinear or exhibits complex patterns, polynomial regression can provide a better fit to the data compared to linear regression.\n",
    "\n",
    "Improved Accuracy: In cases where a linear model doesn't adequately describe the data, using a polynomial model can lead to more accurate predictions and a reduction in the residual errors.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Polynomial regression models, especially those with high-degree polynomials, are prone to overfitting, where the model captures noise in the data and performs poorly on new, unseen data.\n",
    "\n",
    "Increased Complexity: The more terms and higher degrees of polynomials are used, the more complex the model becomes. Interpreting the model's coefficients and relationships can become challenging.\n",
    "\n",
    "Loss of Interpretability: As the model complexity increases, it becomes harder to interpret the coefficients, and the relationship between the variables may not have a straightforward, intuitive explanation.\n",
    "\n",
    "Data Transformation: The independent variable often needs to be transformed by raising it to different powers, which can complicate the modeling process and may require domain knowledge to determine the appropriate transformations.\n",
    "\n",
    "When to Use Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a valuable tool in several situations:\n",
    "\n",
    "Non-linear Relationships: When it's evident that the relationship between the dependent and independent variables is non-linear and a simple linear model doesn't fit well.\n",
    "\n",
    "Complex Patterns: When the data exhibits complex patterns, such as curves, peaks, valleys, or cyclical behavior.\n",
    "\n",
    "Data Exploration: Polynomial regression can be useful in the exploratory phase of data analysis to understand the underlying patterns and relationships in the data.\n",
    "\n",
    "Engineering and Domain Knowledge: When domain knowledge suggests that a particular polynomial relationship is expected or makes sense in the context of the problem.\n",
    "\n",
    "Data Size: With a sufficiently large dataset, the risk of overfitting can be mitigated, making it more feasible to use polynomial regression effectively.\n",
    "\n",
    "In practice, it's crucial to strike a balance between model complexity and predictive accuracy. Careful model selection, cross-validation, and regularization techniques (such as Ridge or Lasso regression) can help address the challenges associated with polynomial regression and make it a powerful tool for modeling non-linear relationships when used judiciously.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6e4c3-8641-4262-87b2-f8accffd5b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3e6d9-d394-44f0-8059-9e08d1f31865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81c3f1-af02-4964-87c0-8fa1723f5d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab7d30-5e8c-4929-a014-9b1a97f99698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
