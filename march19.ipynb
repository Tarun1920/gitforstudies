{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c82642-50cb-4c78-9ddc-75f929728813",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5902d7-37a6-4f9e-9cbd-f156f59e99b3",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to transform numerical features (variables) within a specific range. It scales the data so that it falls within a predefined range, typically between 0 and 1. The formula for Min-Max scaling is as follows:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "norm\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "norm\n",
    "​\n",
    "  is the normalized value.\n",
    "�\n",
    "X is the original data point.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value in the dataset for that feature.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value in the dataset for that feature.\n",
    "The result of Min-Max scaling is that the minimum value in the dataset is transformed to 0, the maximum value is transformed to 1, and all other values are scaled proportionally in between.\n",
    "\n",
    "Example:\n",
    "Let's say you have a dataset of exam scores that range from 60 to 95. You want to scale these scores to a 0-1 range using Min-Max scaling. Here's how you would do it:\n",
    "\n",
    "Original scores:\n",
    "\n",
    "Score 1: 60\n",
    "Score 2: 75\n",
    "Score 3: 85\n",
    "Score 4: 95\n",
    "First, calculate \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "min\n",
    "​\n",
    "  and \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "max\n",
    "​\n",
    " :\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "60\n",
    "X \n",
    "min\n",
    "​\n",
    " =60\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "95\n",
    "X \n",
    "max\n",
    "​\n",
    " =95\n",
    "Now, apply the Min-Max scaling formula to each score:\n",
    "\n",
    "For Score 1:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "1\n",
    "=\n",
    "60\n",
    "−\n",
    "60\n",
    "95\n",
    "−\n",
    "60\n",
    "=\n",
    "0\n",
    "X \n",
    "norm1\n",
    "​\n",
    " = \n",
    "95−60\n",
    "60−60\n",
    "​\n",
    " =0\n",
    "\n",
    "For Score 2:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "=\n",
    "75\n",
    "−\n",
    "60\n",
    "95\n",
    "−\n",
    "60\n",
    "≈\n",
    "0.25\n",
    "X \n",
    "norm2\n",
    "​\n",
    " = \n",
    "95−60\n",
    "75−60\n",
    "​\n",
    " ≈0.25\n",
    "\n",
    "For Score 3:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "3\n",
    "=\n",
    "85\n",
    "−\n",
    "60\n",
    "95\n",
    "−\n",
    "60\n",
    "≈\n",
    "0.75\n",
    "X \n",
    "norm3\n",
    "​\n",
    " = \n",
    "95−60\n",
    "85−60\n",
    "​\n",
    " ≈0.75\n",
    "\n",
    "For Score 4:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "4\n",
    "=\n",
    "95\n",
    "−\n",
    "60\n",
    "95\n",
    "−\n",
    "60\n",
    "=\n",
    "1\n",
    "X \n",
    "norm4\n",
    "​\n",
    " = \n",
    "95−60\n",
    "95−60\n",
    "​\n",
    " =1\n",
    "\n",
    "So, after Min-Max scaling, the scores are transformed to the following range:\n",
    "\n",
    "Score 1: 0\n",
    "Score 2: 0.25\n",
    "Score 3: 0.75\n",
    "Score 4: 1\n",
    "Min-Max scaling is often used in machine learning and data analysis when you want to ensure that all features have a similar scale, which can be important for algorithms that are sensitive to the magnitude of the input features, such as support vector machines and k-nearest neighbors. It helps prevent features with larger values from dominating the learning process and ensures that all features contribute equally to the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b313e-61ef-4b5f-96fd-2e458e9af184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee290c8-bb13-481c-b894-7a01bab4fec1",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling is also known as \"Normalization\" or \"Vector Normalization.\" It differs from Min-Max scaling in that it scales the values of a feature to create a unit vector, meaning that the magnitude of the vector becomes 1 while preserving the direction. This is particularly useful when you want to emphasize the direction of the data points rather than their absolute values.\n",
    "\n",
    "\n",
    "The Unit Vector technique in feature scaling is also known as \"Normalization\" or \"Vector Normalization.\" It differs from Min-Max scaling in that it scales the values of a feature to create a unit vector, meaning that the magnitude of the vector becomes 1 while preserving the direction. This is particularly useful when you want to emphasize the direction of the data points rather than their absolute values.\n",
    "\n",
    "The formula for scaling a feature using Unit Vector technique is as follows:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "∥\n",
    "�\n",
    "∥\n",
    "X \n",
    "norm\n",
    "​\n",
    " = \n",
    "∥X∥\n",
    "X\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "norm\n",
    "​\n",
    "  is the normalized value.\n",
    "�\n",
    "X is the original data point.\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥X∥ represents the magnitude (Euclidean norm) of the data vector, calculated as \n",
    "�\n",
    "1\n",
    "2\n",
    "+\n",
    "�\n",
    "2\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "2\n",
    "X \n",
    "1\n",
    "2\n",
    "​\n",
    " +X \n",
    "2\n",
    "2\n",
    "​\n",
    " +…+X \n",
    "n\n",
    "2\n",
    "​\n",
    " \n",
    "​\n",
    "  for n-dimensional data.\n",
    "The result is that the values of the feature are divided by their magnitude, ensuring that the feature falls on the unit circle in a multi-dimensional space. The direction and relationships between data points are preserved, but the magnitude is scaled to 1.\n",
    "\n",
    "Example:\n",
    "Let's say you have a dataset of two-dimensional data points (X, Y), and you want to scale them to unit vectors. Here's how you would do it:\n",
    "\n",
    "Original data points:\n",
    "\n",
    "Data Point 1: (3, 4)\n",
    "Data Point 2: (1, 2)\n",
    "First, calculate the magnitude of each data point:\n",
    "\n",
    "For Data Point 1:\n",
    "∥\n",
    "�\n",
    "∥\n",
    "=\n",
    "3\n",
    "2\n",
    "+\n",
    "4\n",
    "2\n",
    "=\n",
    "5\n",
    "∥X∥= \n",
    "3 \n",
    "2\n",
    " +4 \n",
    "2\n",
    " \n",
    "​\n",
    " =5\n",
    "\n",
    "For Data Point 2:\n",
    "∥\n",
    "�\n",
    "∥\n",
    "=\n",
    "1\n",
    "2\n",
    "+\n",
    "2\n",
    "2\n",
    "=\n",
    "5\n",
    "∥X∥= \n",
    "1 \n",
    "2\n",
    " +2 \n",
    "2\n",
    " \n",
    "​\n",
    " = \n",
    "5\n",
    "​\n",
    " \n",
    "\n",
    "Now, apply the Unit Vector scaling formula to each data point:\n",
    "\n",
    "For Data Point 1:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "1\n",
    "=\n",
    "(\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    ")\n",
    "X \n",
    "norm1\n",
    "​\n",
    " =( \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " )\n",
    "\n",
    "For Data Point 2:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "=\n",
    "(\n",
    "1\n",
    "5\n",
    ",\n",
    "2\n",
    "5\n",
    ")\n",
    "X \n",
    "norm2\n",
    "​\n",
    " =( \n",
    "5\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    " , \n",
    "5\n",
    "​\n",
    " \n",
    "2\n",
    "​\n",
    " )\n",
    "\n",
    "After applying the Unit Vector scaling, the data points are transformed into unit vectors. These unit vectors have a magnitude of 1, and the direction of the original data points is preserved.\n",
    "\n",
    "In contrast to Min-Max scaling, Unit Vector scaling does not constrain the values to a specific range (like [0, 1]). It is more about emphasizing the relative relationships between data points and making them comparable in terms of direction. It's commonly used in applications where the magnitude of the data is not as important as the relationships or angles between data points, such as in natural language processing or recommendation systems.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9ce25-6a48-4695-a479-c37781c85677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a3bf5-fe80-428b-a595-f8facfd0200a",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used in data analysis and machine learning. Its primary purpose is to reduce the dimensionality of a dataset while retaining as much of the original variation as possible. PCA achieves this by transforming the original features into a new set of orthogonal variables called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "Standardize the data: It's important to standardize the data (subtract the mean and divide by the standard deviation) so that features with larger scales do not dominate the principal components.\n",
    "\n",
    "Calculate the covariance matrix: PCA calculates the covariance matrix of the standardized data. The covariance matrix provides information about how the features are related to each other.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: PCA decomposes the covariance matrix into eigenvectors and eigenvalues. Eigenvectors represent the directions (principal components) along which the data varies the most, and eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "Select the top k eigenvectors: Sort the eigenvectors by their corresponding eigenvalues in decreasing order. Choose the top k eigenvectors to retain, where k is the desired dimensionality of the reduced dataset.\n",
    "\n",
    "Create a new feature matrix: Form a new feature matrix by taking the dot product of the original data and the selected eigenvectors. This matrix represents the data in the reduced-dimensional space.\n",
    "\n",
    "PCA is often used for visualization, noise reduction, or to improve the performance of machine learning algorithms, especially when dealing with high-dimensional data. By reducing the dimensionality, it can also help in mitigating the curse of dimensionality.\n",
    "\n",
    "Example:\n",
    "Let's say you have a dataset with two features, \"Height\" and \"Weight,\" and you want to reduce it to a single dimension using PCA. Here's a simplified example:\n",
    "\n",
    "Original data:\n",
    "\n",
    "Person 1: (Height, Weight) = (170 cm, 65 kg)\n",
    "Person 2: (Height, Weight) = (160 cm, 55 kg)\n",
    "Person 3: (Height, Weight) = (180 cm, 75 kg)\n",
    "Standardize the data:\n",
    "\n",
    "Mean(Height) = 170 cm\n",
    "Mean(Weight) = 65 kg\n",
    "Standard Deviation(Height) ≈ 8.17 cm\n",
    "Standard Deviation(Weight) ≈ 8.17 kg\n",
    "Calculate the covariance matrix:\n",
    "\n",
    "Var(Height) & Cov(Height, Weight) \\\\\n",
    "Cov(Weight, Height) & Var(Weight)\n",
    "\\end{bmatrix} \\]\n",
    "Compute the eigenvectors and eigenvalues.\n",
    "\n",
    "Select the top eigenvector. Suppose it corresponds to the direction (0.707, 0.707).\n",
    "\n",
    "Create the new feature matrix:\n",
    "\n",
    "Person 1: (Height, Weight) = (0.707 * 5, 0.707 * 35) ≈ (3.54, 24.75)\n",
    "Person 2: (Height, Weight) = (0.707 * -5, 0.707 * -5) ≈ (-3.54, -3.54)\n",
    "Person 3: (Height, Weight) = (0.707 * 15, 0.707 * 5) ≈ (10.61, 3.54)\n",
    "Now you have transformed the original 2D data into a 1D space, retaining most of the variance in the data. This can be useful for visualization or further analysis while reducing the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c67dff-3586-4e96-bd21-28e107c99142",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a97742-a830-41b9-98a2-8d43d194f0f1",
   "metadata": {},
   "source": [
    "\n",
    "PCA (Principal Component Analysis) is closely related to feature extraction, as it is a technique that can be used for feature extraction. Feature extraction is a broader concept that involves transforming the original features in a dataset into a new set of features, often with the aim of reducing dimensionality or enhancing the information content of the data. PCA is a specific method for feature extraction that is commonly used to achieve these goals.\n",
    "\n",
    "Here's the relationship between PCA and feature extraction:\n",
    "\n",
    "Dimensionality Reduction: Both PCA and feature extraction techniques aim to reduce the dimensionality of a dataset. High-dimensional data can lead to increased computational complexity and overfitting in machine learning models. By extracting a smaller set of features, you can represent the data in a more compact form while retaining important information.\n",
    "\n",
    "Information Retention: PCA and other feature extraction methods aim to retain as much of the important information in the data as possible. In the case of PCA, this is achieved by selecting the top principal components that explain most of the variance in the data.\n",
    "\n",
    "Orthogonal Features: PCA creates orthogonal (uncorrelated) features, which can be useful for simplifying the relationships within the data and ensuring that the new features are independent of each other.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset of facial images for facial recognition. Each image contains a large number of pixels (features). You want to reduce the dimensionality of the dataset while preserving the key information for recognition.\n",
    "\n",
    "Original Features: Each pixel in the image is a feature, resulting in a very high-dimensional dataset. For example, a 100x100 pixel image has 10,000 features.\n",
    "\n",
    "PCA as Feature Extraction: You can use PCA to extract a set of principal components (features) that capture the most important variations in the images. For instance, you might find that the first few principal components capture variations in lighting, while later components capture more specific facial features.\n",
    "\n",
    "Reduced-Dimensional Representation: By selecting a subset of the principal components, you can represent each image with a much smaller number of features. For instance, you may choose to keep the top 100 principal components.\n",
    "\n",
    "Recognition: In practice, you can use this reduced-dimensional representation for facial recognition. The reduced features still contain much of the essential facial information, making the recognition process computationally efficient while maintaining good accuracy.\n",
    "\n",
    "In this example, PCA is used as a feature extraction technique to transform high-dimensional image data into a lower-dimensional representation while preserving the critical information required for facial recognition. This reduces computational complexity and can help improve the efficiency of the recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddccc75-6c80-4dd4-89c0-ae89ddba8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11263585-25da-4ed3-859c-508360fb5919",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling on features such as price, rating, and delivery time. Min-Max scaling will ensure that these features are within a consistent range, typically between 0 and 1, which is important for many recommendation algorithms. Here's how you can use Min-Max scaling in this context:\n",
    "\n",
    "Understand the Data: Begin by understanding the characteristics of your dataset, such as the range and distribution of the features (price, rating, delivery time). This will help you decide whether scaling is necessary and what range you want to normalize the features into.\n",
    "\n",
    "Calculate the Minimum and Maximum Values:\n",
    "\n",
    "For the \"price\" feature, find the minimum and maximum prices in your dataset.\n",
    "For the \"rating\" feature, determine the minimum and maximum ratings.\n",
    "For the \"delivery time\" feature, identify the minimum and maximum delivery times.\n",
    "Apply Min-Max Scaling to Each Feature:\n",
    "\n",
    "For each feature, apply the Min-Max scaling formula:\n",
    "    Xnorm = X-Xmin/Xmax-Xmin\n",
    "Where:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "norm\n",
    "​\n",
    "  is the normalized value.\n",
    "�\n",
    "X is the original data point.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value for that feature.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value for that feature.\n",
    "Apply this formula to \"price,\" \"rating,\" and \"delivery time\" separately.\n",
    "\n",
    "Standardize the Data: Ensure that you standardize (apply Min-Max scaling) to the same range for all features, typically [0, 1]. This ensures that the features have a similar scale and do not dominate each other during the recommendation process.\n",
    "\n",
    "Use the Scaled Data for Recommendation: With the Min-Max scaled features, you can now feed the data into your recommendation system algorithm, whether it's collaborative filtering, content-based filtering, or a hybrid model. The scaled features ensure that the algorithms treat each feature equally and don't give undue weight to one feature due to its original scale.\n",
    "\n",
    "Monitor and Evaluate: After building the recommendation system, continue to monitor its performance and evaluate the recommendations. You may need to fine-tune the scaling or the recommendation algorithm based on user feedback and performance metrics.\n",
    "\n",
    "By applying Min-Max scaling to the \"price,\" \"rating,\" and \"delivery time\" features in your dataset, you ensure that these features have a consistent and normalized scale, making them suitable for recommendation system algorithms. This preprocessing step can contribute to the accuracy and effectiveness of your food delivery recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae4ba0-966e-4e90-956f-87a627373264",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1f76e-33b6-499c-9fdc-5b2410599c1e",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset in a stock price prediction project can be an effective approach, especially when dealing with a large number of features. Here's how you can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Start by gathering and preprocessing your dataset. This includes cleaning and handling missing data, normalizing or standardizing the features (mean centering and scaling to unit variance), and ensuring the data is ready for analysis.\n",
    "Select Features: Determine which features are relevant for your stock price prediction task. Features might include financial indicators (e.g., earnings, revenue, debt-to-equity ratio), market trends (e.g., stock market indices), or any other relevant economic data.\n",
    "\n",
    "Standardize the Data: Standardize your data by subtracting the mean from each feature and dividing by the standard deviation. This step is crucial for PCA, as it ensures that all features are on the same scale.\n",
    "\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized feature matrix. The covariance matrix represents how features are related to each other.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the principal components, and the eigenvalues indicate how much variance is explained by each principal component.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors in descending order based on their corresponding eigenvalues. The principal components with the highest eigenvalues capture the most variance in the data.\n",
    "\n",
    "Select Principal Components: Decide how many principal components you want to retain. You can choose a number that explains a significant portion of the variance in the data. Typically, you might aim to retain a certain percentage of the total variance, such as 95%.\n",
    "\n",
    "Create a Reduced-Dimensional Feature Matrix: Take the top-k eigenvectors (principal components) and create a new feature matrix by projecting the original data onto these components. This results in a reduced-dimensional feature space.\n",
    "\n",
    "Model Building: Train your stock price prediction model using the reduced-dimensional feature matrix. You can use various machine learning algorithms, such as regression models or time series forecasting methods, depending on the nature of your prediction task.\n",
    "\n",
    "Evaluation and Tuning: Evaluate the model's performance and make necessary adjustments. You may need to experiment with different numbers of principal components to find the optimal balance between dimensionality reduction and predictive accuracy.\n",
    "\n",
    "PCA is particularly useful in stock price prediction when you have a large number of potentially correlated features. It can help you reduce noise, focus on the most important features, and avoid overfitting. However, it's important to note that the interpretation of the principal components may not always be straightforward in the context of financial data, as they are linear combinations of the original features. Therefore, domain knowledge remains crucial in understanding the economic significance of the reduced features.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5e65a-b5b0-471c-bac1-23551f5f053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9b3fa-7fef-4f9a-b349-7cb6f1fbfbbf",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform a dataset to a range of -1 to 1, you can use the following formula\n",
    "Xnorm = X- Xmin/Xmax-Xmin *(max - min) + min\n",
    "In this case, you want to scale the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, so \n",
    "min=−1 and \n",
    "max=1. First, you need to find the minimum and maximum values in your dataset.\n",
    "calculate Xmin:\n",
    "Xmin= min(1,5,10,15,20)= 1\n",
    "Xmax = max(1,5,10,15,20)= 20\n",
    "for 1:\n",
    "Xnorm =1-1/20-1 ×(1−(−1))+(−1)= 0/19 * 2−1= −1\n",
    "After applying Min-Max scaling, the transformed dataset [1, 5, 10, 15, 20] in the range -1 to 1 is:\n",
    "[-1, -0.5263, 0.2105, 0.9474, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37799db-4ad6-4108-ab35-bbdbeed2f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9d8e5-9267-48f1-b3ed-f500b7703458",
   "metadata": {},
   "source": [
    "The number of principal components to retain in PCA is a crucial decision and depends on your specific goals and the variance explained by those components. Typically, you aim to retain enough principal components to capture a significant portion of the total variance while reducing dimensionality. Here's how you can determine the number of principal components to retain:\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Perform PCA on your dataset to compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues represent the variance explained by each principal component, and the eigenvectors represent the direction in feature space.\n",
    "\n",
    "Sort Eigenvalues: Arrange the eigenvalues in descending order. The first eigenvalue corresponds to the first principal component, the second eigenvalue to the second principal component, and so on.\n",
    "\n",
    "Explained Variance: Calculate the cumulative explained variance for each principal component. This is the sum of eigenvalues up to a particular component divided by the sum of all eigenvalues. It indicates how much of the total variance in the data is explained by these components.\n",
    "\n",
    "Threshold or Rule of Thumb: Decide on a threshold or rule of thumb for the amount of variance you want to retain. Common thresholds include retaining enough principal components to explain 95% or 99% of the total variance. Alternatively, you can look at the \"elbow\" point in the explained variance plot to make a decision.\n",
    "\n",
    "Select the Number of Principal Components: Based on your chosen threshold or rule of thumb, count the number of principal components needed to meet that criterion.\n",
    "\n",
    "In the context of your dataset containing features like height, weight, age, gender, and blood pressure, the number of principal components to retain would depend on several factors:\n",
    "\n",
    "The total number of features in the original dataset.\n",
    "The correlations or relationships between these features.\n",
    "The amount of variance you want to retain.\n",
    "Keep in mind that PCA is typically used to reduce dimensionality while retaining most of the information. However, the decision on the number of principal components to retain may vary based on your specific goals. If, for example, you want to reduce dimensionality for visualization or interpretability while retaining as much information as possible, you might choose to retain a higher number of components. On the other hand, if computational efficiency is a concern, you might choose a smaller number of components.\n",
    "\n",
    "It's not possible to determine the exact number of components without knowledge of the data and your specific objectives. You may need to experiment with different numbers of components and assess the trade-off between dimensionality reduction and information retention.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
