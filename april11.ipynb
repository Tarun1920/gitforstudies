{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef19c31-115d-4613-af0b-c29060ed10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce76fcd-56e8-47df-8dec-658934164922",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning involves combining multiple individual models to create a more powerful and accurate predictive model. The idea behind ensemble methods is that by combining the predictions of several models, the overall result can be more robust and accurate than any individual model on its own.\n",
    "\n",
    "Ensemble techniques work on the principle of \"wisdom of the crowd\" or the idea that a group of models working together will perform better than any single model. There are several types of ensemble techniques, with some of the most popular ones being:\n",
    "\n",
    "Bagging (Bootstrap Aggregating): This method involves training multiple models independently on different random subsets of the training data and then combining their predictions, often using techniques like averaging or voting.\n",
    "\n",
    "Boosting: Boosting is an iterative technique where models are trained sequentially, with each model correcting the errors made by its predecessors. Popular algorithms like AdaBoost, Gradient Boosting, and XGBoost fall under this category.\n",
    "\n",
    "Random Forest: This is a type of ensemble method that combines multiple decision trees. Each tree is trained on a different random subset of the training data and uses a random subset of features. The final prediction is the average or majority vote of these individual trees.\n",
    "\n",
    "Stacking: Stacking involves training multiple different models and then combining their predictions using another model, often called a meta-learner or blender. The idea is to learn how to combine the predictions of the individual models to make a final prediction.\n",
    "\n",
    "Ensemble methods are widely used in machine learning because they often result in improved accuracy, generalization, and robustness compared to using a single model. However, they also require more computational resources and can be more complex to implement compared to individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4540d-dad9-4b48-b8db-3b284064a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a8280-3b1d-4311-93cd-775c25697be6",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "\n",
    "Improved Predictive Performance: Ensemble methods often lead to better predictive performance compared to individual models. By combining the predictions of multiple models, the ensemble can capture different aspects of the data and reduce the risk of overfitting, resulting in more accurate and robust predictions.\n",
    "\n",
    "Reduction of Overfitting: Individual machine learning models can sometimes overfit the training data, meaning they perform well on the training data but poorly on unseen data. Ensembles, particularly bagging and boosting methods, can reduce overfitting by averaging out the errors or by focusing on correcting each other's mistakes.\n",
    "\n",
    "Increased Robustness: Ensemble techniques are less sensitive to noise and outliers in the data. Since they combine multiple models, the influence of individual noisy predictions is often diminished, making the ensemble more robust to data imperfections.\n",
    "\n",
    "Model Diversity: Ensembles work best when the individual models are diverse, meaning they make different types of errors. By combining models with different strengths and weaknesses, an ensemble can benefit from the collective knowledge of the individual models.\n",
    "\n",
    "Better Generalization: Ensembles are more likely to generalize well to new, unseen data. This is particularly valuable in scenarios where the data distribution may change over time, as ensembles can adapt more effectively.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide range of machine learning algorithms, making them versatile for solving various types of problems, such as classification, regression, and even anomaly detection.\n",
    "\n",
    "Handling Imbalanced Data: Ensembles can help address class imbalance issues in classification tasks by giving more weight to the minority class, improving the model's ability to correctly classify rare events.\n",
    "\n",
    "Reducing Bias: Ensembles can reduce the bias introduced by using a single model, as different models may have different biases. Combining them can help mitigate these biases.\n",
    "\n",
    "State-of-the-Art Performance: In many machine learning competitions and real-world applications, ensemble techniques have been responsible for achieving state-of-the-art results in terms of predictive accuracy.\n",
    "\n",
    "While ensemble techniques offer many advantages, they also come with some trade-offs, such as increased computational complexity and the need for more data. The choice of the specific ensemble method depends on the problem at hand and the characteristics of the data, as different methods may be more suitable for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756ae5c-bb5a-4e2c-85ef-b3e223d7e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669268e4-1c47-4d86-b613-dbd7cd3d29bc",
   "metadata": {},
   "source": [
    "Bagging, which stands for \"Bootstrap Aggregating,\" is an ensemble machine learning technique that aims to improve the accuracy and robustness of predictive models. It does so by training multiple instances of the same base model on different random subsets of the training data and then combining their predictions. Bagging is a widely used technique and is particularly effective for reducing variance and overfitting.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "Bootstrap Sampling: Bagging starts by creating multiple random subsets of the training data through a process called bootstrap sampling. This involves randomly selecting data points with replacement from the original training dataset. As a result, some data points may appear multiple times in a subset, while others may not appear at all.\n",
    "\n",
    "Base Model Training: For each of these bootstrap samples, a base model (usually a decision tree, but it can be other models as well) is trained independently on the corresponding subset of data. Since each subset is slightly different due to the random sampling, the base models will have different perspectives on the data.\n",
    "\n",
    "Combining Predictions: After training all the base models, bagging combines their predictions to make a final prediction. The most common methods for combining predictions in bagging are averaging (for regression problems) or voting (for classification problems). In regression, you average the predictions of all the base models, while in classification, you take a majority vote.\n",
    "\n",
    "The key benefits of bagging are as follows:\n",
    "\n",
    "Variance Reduction: Bagging helps reduce the variance of the model by averaging or voting over multiple independent models. This is especially beneficial when the base models are prone to overfitting the training data.\n",
    "\n",
    "Increased Robustness: By training on different subsets of data, bagging makes the model more robust to variations and noise in the dataset, improving its generalization to unseen data.\n",
    "\n",
    "Improved Accuracy: In many cases, bagging can lead to better predictive performance compared to a single model.\n",
    "\n",
    "The most well-known and widely used bagging algorithm is the Random Forest, which employs bagging with decision trees as base models. Random Forests further enhance the bagging approach by introducing randomness in the feature selection during tree construction, which increases the diversity of the base models.\n",
    "\n",
    "Bagging is a versatile ensemble technique and can be applied to a variety of machine learning algorithms, not just decision trees. It is an effective tool for improving model performance and robustness, especially in situations where overfitting is a concern.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f56b1-f5ee-413f-ac88-e947cef6e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6991a-e3c9-4e45-a842-e6bd307f9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is another ensemble machine learning technique that is used to improve the performance of predictive models. Unlike bagging, which trains multiple models independently and then combines their predictions, boosting builds a sequence of models, each one correcting the errors of its predecessor. The idea is to focus on the instances that are more challenging to classify correctly and give them more attention in each iteration.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "Base Model Training: Boosting starts by training a base model on the original training data.\n",
    "\n",
    "Instance Weighting: After the first model is trained, boosting assigns weights to each instance in the training dataset. Initially, all instances have equal weights. However, the instances that the model misclassifies or struggles to predict correctly are given higher weights, making them more important in the subsequent iterations.\n",
    "\n",
    "Sequential Model Building: Boosting builds a sequence of models, where each model is trained to focus more on the instances with higher weights (i.e., the ones that the previous model found challenging). These models are often referred to as \"weak learners\" because they don't need to be highly accurate on their own.\n",
    "\n",
    "Weight Adjustment: After each model is trained, the instance weights are updated based on the errors made by the model. The instances that were misclassified are assigned higher weights, increasing their importance in the next iteration.\n",
    "\n",
    "Combining Predictions: The final prediction is made by combining the predictions of all the models. Typically, boosting uses weighted voting, where models with better performance are given more influence in the final prediction.\n",
    "\n",
    "The key benefits of boosting are as follows:\n",
    "\n",
    "Improvement of Weak Models: Boosting can convert a collection of weak models into a strong, highly accurate model by iteratively focusing on the instances that are challenging to classify.\n",
    "\n",
    "Reduced Bias: Boosting can reduce bias and error by learning from the mistakes made by the previous models in the ensemble.\n",
    "\n",
    "Adaptability: Boosting is adaptive in the sense that it can adjust to the complexity of the data. It keeps learning and improving until it minimizes the training error or achieves a predefined performance level.\n",
    "\n",
    "Some popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost. These algorithms differ in their strategies for updating instance weights and building the ensemble of models. Boosting is widely used in various machine learning applications and is known for its ability to achieve high predictive accuracy. However, it can be more sensitive to noisy data compared to bagging techniques like Random Forest.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d874cad-b43a-4289-9f75-55df1123e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a70ad9-f9c5-4ee3-b259-970ab615ee09",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits when used in machine learning:\n",
    "\n",
    "Improved Predictive Performance: One of the primary benefits of ensemble techniques is their ability to improve predictive performance. By combining multiple models, ensembles can capture different aspects of the data, leading to more accurate and robust predictions. This often results in better model performance compared to using a single model.\n",
    "\n",
    "Reduction of Overfitting: Ensembles, particularly bagging and boosting methods, can help reduce overfitting. Overfitting occurs when a model fits the training data too closely, leading to poor generalization on unseen data. By combining models with different perspectives and errors, ensembles can mitigate overfitting and produce more stable models.\n",
    "\n",
    "Increased Robustness: Ensembles are less sensitive to noise and outliers in the data. Since they combine predictions from multiple models, the influence of individual erroneous predictions is typically reduced, making the ensemble more robust to imperfections in the data.\n",
    "\n",
    "Better Generalization: Ensembles are more likely to generalize well to new, unseen data. This is particularly valuable in real-world applications where the data distribution may change over time or in scenarios where the training data is limited.\n",
    "\n",
    "Model Diversity: Ensembles work best when the individual models are diverse, meaning they make different types of errors. By combining models with different strengths and weaknesses, ensembles can benefit from the collective knowledge of the individual models.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide range of machine learning algorithms, making them versatile for solving various types of problems, such as classification, regression, and even anomaly detection.\n",
    "\n",
    "Handling Imbalanced Data: Ensembles can help address class imbalance issues in classification tasks by giving more weight to the minority class, improving the model's ability to correctly classify rare events.\n",
    "\n",
    "Reducing Bias: Ensembles can reduce the bias introduced by using a single model, as different models may have different biases. Combining them can help mitigate these biases.\n",
    "\n",
    "State-of-the-Art Performance: In many machine learning competitions and real-world applications, ensemble techniques have been responsible for achieving state-of-the-art results in terms of predictive accuracy.\n",
    "\n",
    "Interpretability: Some ensemble techniques, like Random Forest, can provide feature importance scores, which can be valuable for understanding the factors that drive predictions.\n",
    "\n",
    "While ensemble techniques offer numerous benefits, it's important to note that they also come with some trade-offs, such as increased computational complexity and the need for more data. The choice of the specific ensemble method depends on the problem at hand and the characteristics of the data, as different methods may be more suitable for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374bfdd-fe61-480c-ad89-eb4eddcfd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e831825-4732-48b4-b61b-c6c8fadf1ad1",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning, and they often lead to improved predictive performance and model robustness compared to individual models. However, whether ensemble techniques are always better than individual models depends on several factors, and there are situations where individual models may be more appropriate or sufficient. Here are some considerations:\n",
    "\n",
    "Data Size: Ensembles typically require more data to be effective. If you have a very small dataset, you might not have enough data to train multiple models, and individual models may perform better.\n",
    "\n",
    "Model Complexity: If your individual model is already very complex and has high capacity (e.g., a deep neural network with many layers), it might not benefit significantly from ensembling. Ensembles are often more effective when the base models are relatively simple and diverse.\n",
    "\n",
    "Computational Resources: Ensembling can be computationally expensive, as it involves training multiple models. In cases where computational resources are limited, it may be more practical to rely on a single model.\n",
    "\n",
    "Time Constraints: If there are strict time constraints for making predictions, ensembling may introduce additional processing time due to the need to combine the predictions of multiple models. Individual models may be more suitable when real-time or low-latency predictions are required.\n",
    "\n",
    "Model Interpretability: Ensembles can be more challenging to interpret than individual models. If model interpretability is a primary concern in your application, using a single model may be preferable.\n",
    "\n",
    "Data Quality and Noise: Ensembles can be sensitive to noisy data or outliers. If your dataset contains a lot of noise, ensembles may not always perform better, as they can potentially amplify the noise from different models.\n",
    "\n",
    "Type of Problem: The nature of the problem you're trying to solve can influence whether ensembles are beneficial. Some problems may naturally benefit from ensembling (e.g., image classification), while others may not show significant improvements (e.g., simple rule-based problems).\n",
    "\n",
    "Algorithm Selection: The choice of the ensemble technique matters. Some ensemble methods, like Random Forest, are known for their robustness and can work well in a wide range of scenarios. However, the performance of boosting methods, for example, can be highly dependent on the choice of hyperparameters.\n",
    "\n",
    "In summary, while ensemble techniques offer many advantages, they are not a one-size-fits-all solution. It's essential to consider the specific characteristics of your data, the nature of the problem, available resources, and your goals when deciding whether to use ensemble techniques or stick with individual models. In practice, it's often a good idea to experiment with both approaches to determine which one yields the best results for your particular machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1adeb0-b924-4331-9bab-6c3f26147b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c922d-43c7-494c-8317-be4219a81cd1",
   "metadata": {},
   "source": [
    "A confidence interval can be calculated using the bootstrap resampling technique to estimate the uncertainty of a statistic or parameter from a sample. Here's a step-by-step process for calculating a confidence interval using bootstrap:\n",
    "\n",
    "Collect Your Data: Start with your original dataset, which contains the sample you want to estimate the parameter for.\n",
    "\n",
    "Choose a Resampling Size: Determine the number of resamples (bootstrap samples) you want to generate. Common choices include 1,000 or 10,000 resamples, but you can adjust this based on your computational resources and the precision you require.\n",
    "\n",
    "Resample with Replacement: For each bootstrap sample, randomly select data points from your original dataset with replacement. This means that each bootstrap sample can contain the same data point multiple times, and some data points may not be included at all.\n",
    "\n",
    "Calculate the Statistic: Apply the statistic or parameter estimation method you're interested in to each of the bootstrap samples. This could be the mean, median, standard deviation, or any other relevant statistic.\n",
    "\n",
    "Obtain the Bootstrap Sampling Distribution: After calculating the statistic for each bootstrap sample, you will have a distribution of the statistic's values. This is called the bootstrap sampling distribution.\n",
    "\n",
    "Determine the Confidence Level: Choose the desired confidence level for your interval. Common choices are 95%, 90%, or 99%, but you can select any confidence level that suits your needs.\n",
    "\n",
    "Calculate the Confidence Interval: To calculate the confidence interval, you need to find the appropriate percentiles from the bootstrap sampling distribution. For example, for a 95% confidence interval, you would find the 2.5th percentile and the 97.5th percentile of the bootstrap sampling distribution. These percentiles represent the lower and upper bounds of the confidence interval, respectively.\n",
    "\n",
    "Mathematically, the confidence interval can be expressed as follows:\n",
    "\n",
    "Lower Bound = Percentile(Bootstrap Sampling Distribution, (1 - α/2))\n",
    "Upper Bound = Percentile(Bootstrap Sampling Distribution, (α/2))\n",
    "\n",
    "Where α is the significance level (1 - confidence level), and the percentiles are chosen based on the desired confidence level. For a 95% confidence interval, α/2 is 0.025, and you would find the 2.5th and 97.5th percentiles.\n",
    "\n",
    "Report the Confidence Interval: Finally, report the calculated confidence interval, which provides an estimate of the range within which the true parameter or statistic is likely to fall with the specified confidence level.\n",
    "Bootstrap resampling is a powerful method for estimating confidence intervals, especially when the underlying distribution of the data is unknown or complex. It provides a way to assess the uncertainty associated with a statistic or parameter, making it a valuable tool in statistical analysis and hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae6656-db73-460b-ba5d-e8ae43c44455",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ab835-f996-4408-935d-fddb876ae81a",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic or parameter from a given sample. It is particularly useful when you want to make inferences about a population or a dataset but have limited data. Bootstrap works by creating multiple resamples (with replacement) from the original sample, allowing you to perform statistical analyses and estimate various properties. Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "Original Sample (Step 0):\n",
    "\n",
    "Start with your original dataset, which is the sample from which you want to estimate statistics or parameters. This dataset contains \"n\" data points.\n",
    "Resample (Step 1):\n",
    "\n",
    "Randomly select \"n\" data points from the original sample with replacement. Since you're allowed to choose the same data point multiple times and some data points may be left out, the resampled dataset is likely to be slightly different from the original one.\n",
    "Statistic Calculation (Step 2):\n",
    "\n",
    "Calculate the statistic of interest (e.g., mean, median, variance, or any other parameter) for the resampled dataset.\n",
    "Repeat Steps 1 and 2 (Step 3):\n",
    "\n",
    "Repeat Steps 1 and 2 a large number of times (usually thousands or tens of thousands) to generate multiple resampled datasets and calculate the statistic for each of them.\n",
    "Bootstrap Sampling Distribution (Step 4):\n",
    "\n",
    "After generating a large number of statistics from the resampled datasets, you create a distribution of these statistics. This distribution is referred to as the \"bootstrap sampling distribution.\"\n",
    "Statistical Inference (Step 5):\n",
    "\n",
    "You can use the bootstrap sampling distribution to make inferences about the population or the true data distribution. For example, you can estimate the mean, variance, confidence intervals, standard error, or even the distribution of a parameter based on the bootstrap statistics.\n",
    "Bootstrap is a powerful and versatile technique that can be applied to a wide range of statistical problems. It provides a way to estimate properties of a statistic or parameter, assess uncertainty, and make statistical inferences without making strong assumptions about the underlying data distribution. It is widely used in hypothesis testing, confidence interval estimation, and other statistical analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9da41-25b5-40cc-8abe-fa7ed5ff05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ea48d-80d2-4ef0-9524-d902966ffc4c",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using the bootstrap method, you can follow these steps:\n",
    "\n",
    "Collect the Original Sample:\n",
    "\n",
    "The original sample consists of 50 measurements of tree heights, with a mean of 15 meters and a standard deviation of 2 meters.\n",
    "Resampling:\n",
    "\n",
    "Generate a large number of resampled datasets (often thousands or tens of thousands) by randomly selecting 50 measurements from the original sample with replacement. This creates a set of resampled datasets.\n",
    "Calculate the Sample Mean:\n",
    "\n",
    "For each resampled dataset, calculate the mean height.\n",
    "Bootstrap Sampling Distribution:\n",
    "\n",
    "After calculating the mean height for each resampled dataset, you'll have a distribution of sample means. This distribution is your bootstrap sampling distribution.\n",
    "Calculate Percentiles for Confidence Interval:\n",
    "\n",
    "To find the 95% confidence interval, you need to identify the 2.5th and 97.5th percentiles of the bootstrap sampling distribution. These percentiles correspond to the lower and upper bounds of the confidence interval.\n",
    "Confidence Interval Calculation:\n",
    "\n",
    "Calculate the confidence interval by determining the values at the 2.5th and 97.5th percentiles of the bootstrap sampling distribution.\n",
    "Here's how to calculate the confidence interval for the population mean height using the information provided:\n",
    "\n",
    "Resample the data many times (e.g., 10,000 times), calculating the mean height for each resample.\n",
    "\n",
    "Create a distribution of these sample means from the resampling.\n",
    "\n",
    "Find the 2.5th and 97.5th percentiles of this distribution to obtain the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "The R code for performing bootstrap in this case can be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba6b7f-3fff-4779-bd63-0d61bdb43155",
   "metadata": {},
   "source": [
    "# Define the original sample\n",
    "original_sample <- c(15, 15, 14.5, 16, 14, 16.5, 15.5, 15, 14, 15, 14.5, 16.5, 15, 14, 16, 15, 14.5, 15, 15.5, 16,\n",
    "                    15.5, 14, 15, 15, 15.5, 16.5, 14, 16, 15.5, 15, 16, 15, 15.5, 14, 16, 15, 15, 15, 14, 15, 15,\n",
    "                    16.5, 14, 15.5, 15.5, 15.5, 14, 15.5, 15, 15)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples <- 10000\n",
    "\n",
    "# Initialize an empty vector to store the sample means\n",
    "bootstrap_means <- numeric(num_samples)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "set.seed(123) # For reproducibility\n",
    "for (i in 1:num_samples) {\n",
    "  resampled_data <- sample(original_sample, replace = TRUE)\n",
    "  bootstrap_means[i] <- mean(resampled_data)\n",
    "}\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval <- quantile(bootstrap_means, c(0.025, 0.975))\n",
    "\n",
    "# Print the confidence interval\n",
    "cat(\"95% Confidence Interval for Mean Height: [\", round(confidence_interval[1], 2), \"m, \", round(confidence_interval[2], 2), \"m]\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78de4da-95fc-4a7f-8ccc-bd4557515ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dff95a-9121-45a2-b234-bc7cd3c491e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125b30f-663f-4be0-9872-e4b0c90beed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
