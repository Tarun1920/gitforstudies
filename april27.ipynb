{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40df1af-e494-4ef3-8a39-766962974159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764d0f3-2f8a-4450-ac2d-07d3926ab4e3",
   "metadata": {},
   "source": [
    "Clustering algorithms are unsupervised machine learning techniques used to group similar data points into clusters or groups based on their similarity or distance from each other. There are several types of clustering algorithms, each with its own approach and underlying assumptions. Here are some of the most common types of clustering algorithms:\n",
    "\n",
    "K-Means Clustering:\n",
    "\n",
    "Approach: K-Means is a partitioning clustering algorithm that aims to divide the data into 'K' clusters, where 'K' is a user-specified parameter.\n",
    "Assumptions: It assumes that clusters are spherical and have approximately equal sizes. It works well when clusters are relatively compact and separated.\n",
    "Hierarchical Clustering:\n",
    "\n",
    "Approach: Hierarchical clustering creates a tree-like structure of clusters, starting with each data point as a single cluster and merging them step by step.\n",
    "Assumptions: It doesn't assume a fixed number of clusters, making it more flexible. The tree structure can be cut at different levels to obtain different numbers of clusters.\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "\n",
    "Approach: DBSCAN identifies dense regions of data points as clusters and is capable of finding clusters of arbitrary shapes.\n",
    "Assumptions: It does not assume a fixed number of clusters and can discover clusters of different sizes and shapes. It assumes that clusters have higher point density compared to the background.\n",
    "Agglomerative Clustering:\n",
    "\n",
    "Approach: Agglomerative clustering is a bottom-up hierarchical clustering technique. It starts with each data point as a single cluster and iteratively merges the closest clusters.\n",
    "Assumptions: It doesn't assume a fixed number of clusters and allows you to obtain a hierarchical representation of the data.\n",
    "Gaussian Mixture Model (GMM):\n",
    "\n",
    "Approach: GMM assumes that the data points are generated from a mixture of Gaussian distributions and uses probabilistic models to assign data points to clusters.\n",
    "Assumptions: It assumes that data points are generated from a combination of Gaussian distributions and is effective for modeling clusters with different shapes.\n",
    "Spectral Clustering:\n",
    "\n",
    "Approach: Spectral clustering uses the eigenvalues of a similarity matrix to group data points into clusters.\n",
    "Assumptions: It doesn't make strong assumptions about the shape or size of clusters and can be effective in capturing complex structures in the data.\n",
    "Fuzzy Clustering:\n",
    "\n",
    "Approach: Fuzzy clustering allows data points to belong to multiple clusters with varying degrees of membership.\n",
    "Assumptions: It relaxes the hard assignment of data points to clusters and is suitable when data points can belong to multiple clusters simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d908c9d-374b-41e3-9ca2-78cc3b7dba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75240463-af4c-41d6-bda1-b3b03795e889",
   "metadata": {},
   "source": [
    "K-Means clustering is one of the most popular and widely used unsupervised machine learning algorithms for partitioning a dataset into clusters or groups based on the similarity of data points. It is a centroid-based clustering algorithm and works as follows:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Choose the number of clusters, denoted as 'K,' that you want to divide the data into.\n",
    "Randomly select 'K' data points as initial cluster centroids. These initial centroids can be any data points from the dataset.\n",
    "Assignment:\n",
    "\n",
    "For each data point in the dataset, calculate its distance to each of the 'K' centroids. Common distance measures include Euclidean distance, Manhattan distance, or other similarity measures.\n",
    "Assign each data point to the cluster whose centroid is closest to it. This results in K clusters, with each data point belonging to the cluster whose centroid it is nearest to.\n",
    "Update:\n",
    "\n",
    "Recalculate the centroids of each cluster by taking the mean of all data points assigned to that cluster. The centroid is a new data point representing the center of the cluster.\n",
    "Convergence:\n",
    "\n",
    "Repeat the Assignment and Update steps iteratively until one of the stopping criteria is met. Common stopping criteria include no change in cluster assignments or a maximum number of iterations.\n",
    "Final Clusters:\n",
    "\n",
    "Once the algorithm converges, you have your final clusters, and each data point is assigned to one of these clusters.\n",
    "Key points to note about K-Means clustering:\n",
    "\n",
    "K-Means is a non-hierarchical, partitioning clustering algorithm, meaning it divides the data into a fixed number of non-overlapping clusters.\n",
    "The number of clusters (K) needs to be specified in advance, which can be a challenge if you don't have prior knowledge about the data.\n",
    "K-Means is sensitive to the initial placement of centroids, and different initializations can lead to different results. Therefore, it is common to run the algorithm multiple times with different initializations and choose the best result based on a criterion like the sum of squared distances (inertia) from data points to their assigned centroids.\n",
    "K-Means is suitable for datasets with well-defined, roughly spherical clusters, but it may not perform well on data with irregularly shaped or overlapping clusters.\n",
    "K-Means is relatively fast and works well for many real-world clustering tasks, making it a popular choice for data analysis and segmentation. However, its performance can be limited by the choice of K and its sensitivity to initializations, which is why it's often used in conjunction with techniques to select an optimal K and improve the initialization process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab309544-ed00-4a4d-a377-898eaa74d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d07a3-df5b-4dab-b9c4-207f2b100961",
   "metadata": {},
   "source": [
    "K-Means clustering is a widely used clustering technique with several advantages and limitations compared to other clustering algorithms. Here are some of the key advantages and limitations of K-Means clustering:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: K-Means is a straightforward and easy-to-understand algorithm. It's relatively simple to implement and is computationally efficient, making it suitable for large datasets.\n",
    "\n",
    "Speed: K-Means is computationally efficient and can handle large datasets with many features. It converges quickly, which is important for real-time or interactive applications.\n",
    "\n",
    "Scalability: K-Means is capable of handling datasets with a large number of data points and clusters.\n",
    "\n",
    "Interpretability: The resulting clusters in K-Means are easy to interpret and describe, as they are formed around centroids.\n",
    "\n",
    "Deterministic: K-Means is a deterministic algorithm, which means that given the same data and initializations, it will produce the same results each time it is run.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Sensitivity to Initialization: K-Means can produce different results based on the initial placement of cluster centroids. This sensitivity can lead to suboptimal clustering results.\n",
    "\n",
    "Assumption of Spherical Clusters: K-Means assumes that clusters are spherical and equally sized, which may not hold in real-world data where clusters can have irregular shapes and different sizes.\n",
    "\n",
    "Fixed Number of Clusters (K): You need to specify the number of clusters (K) in advance, which can be a challenge if you don't have prior knowledge of the data's underlying structure.\n",
    "\n",
    "Not Suitable for Non-Globular Clusters: K-Means may perform poorly when dealing with data containing non-globular or complex cluster shapes. In such cases, other clustering algorithms like DBSCAN or hierarchical clustering may be more appropriate.\n",
    "\n",
    "Sensitive to Outliers: Outliers can significantly impact K-Means results, as they can distort the centroid positions and lead to incorrect cluster assignments.\n",
    "\n",
    "Scaling Sensitivity: The algorithm's performance can be affected by the scaling and units of the features, so preprocessing the data may be necessary.\n",
    "\n",
    "Empty Clusters: In some cases, K-Means may produce empty clusters if the data is not well-suited to the chosen value of K.\n",
    "\n",
    "Local Optima: K-Means can converge to local optima, meaning it may not find the best clustering solution in every run. Using multiple initializations and choosing the best result can mitigate this issue.\n",
    "\n",
    "Influenced by Noise: K-Means can be sensitive to noisy data, and noisy points may be assigned to clusters even if they don't belong.\n",
    "\n",
    "In summary, K-Means clustering is a powerful and widely used technique with its advantages, particularly its simplicity and speed. However, its limitations, such as sensitivity to initialization and assumptions about cluster shape, should be considered when choosing a clustering algorithm for a specific task. Depending on the nature of the data and the clustering goals, other algorithms like hierarchical clustering, DBSCAN, or Gaussian Mixture Models may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cdc8e-67e1-403e-a579-550c0dca1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5cdd7-a37c-42a0-866a-b843b00ffb02",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters in K-Means clustering is a crucial step, as choosing the wrong number of clusters can lead to suboptimal results. Several methods can help you find the optimal number of clusters. Here are some common approaches:\n",
    "\n",
    "Elbow Method:\n",
    "\n",
    "The Elbow Method involves running the K-Means algorithm for a range of values of K and plotting the sum of squared distances (inertia) between data points and their assigned centroids for each K.\n",
    "The point where the inertia starts to show diminishing returns, resembling an \"elbow\" in the plot, is often considered the optimal K.\n",
    "Keep in mind that the \"elbow\" is not always well-defined, and the choice of K can be somewhat subjective.\n",
    "Silhouette Score:\n",
    "\n",
    "The Silhouette Score measures how similar each data point is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "Calculate the Silhouette Score for different values of K and choose the K that maximizes this score.\n",
    "A higher Silhouette Score indicates that the data points are well-clustered.\n",
    "Gap Statistics:\n",
    "\n",
    "Gap Statistics compare the clustering quality of your K-Means model to that of a random data distribution.\n",
    "By comparing the observed within-cluster variation to the expected variation, you can find the K that maximizes the gap, indicating the optimal number of clusters.\n",
    "Davies-Bouldin Index:\n",
    "\n",
    "The Davies-Bouldin Index quantifies the average similarity between each cluster and its most similar cluster, where lower values indicate better clustering.\n",
    "You can calculate the Davies-Bouldin Index for different values of K and choose the K with the lowest index.\n",
    "Silhouette Analysis:\n",
    "\n",
    "Silhouette Analysis involves calculating the Silhouette Score for a range of K values and visually inspecting the Silhouette plots.\n",
    "Look for K values with higher average silhouette widths and a well-defined silhouette shape, which indicates better clustering.\n",
    "Gap Statistic with Bootstrapping:\n",
    "\n",
    "This method extends the gap statistics by incorporating bootstrapping to obtain more reliable results. It helps reduce the uncertainty in the gap statistics estimation.\n",
    "Calinski-Harabasz Index:\n",
    "\n",
    "The Calinski-Harabasz Index, also known as the Variance Ratio Criterion, measures the ratio of between-cluster variance to within-cluster variance.\n",
    "Higher values indicate more compact and well-separated clusters, so you can choose the K with the highest index.\n",
    "Cross-Validation:\n",
    "\n",
    "You can use cross-validation techniques, such as k-fold cross-validation, to evaluate the performance of K-Means with different values of K.\n",
    "Choose the K that results in the best clustering performance based on a validation metric like the F1 score, adjusted Rand index, or other relevant criteria.\n",
    "Domain Knowledge:\n",
    "\n",
    "In some cases, domain knowledge or business understanding can help determine an appropriate number of clusters. If you have prior knowledge about the data or the problem you're solving, it can guide your choice of K.\n",
    "It's important to note that there is no one-size-fits-all method for determining the optimal number of clusters. The choice of the method may depend on the characteristics of your data and the specific goals of your clustering task. It's often a good practice to try multiple methods and validate the results to ensure that the chosen K makes sense in the context of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ca697-2a12-4c33-acba-b4d269449058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114dbcc0-6557-4f54-82c0-182146d17ba8",
   "metadata": {},
   "source": [
    "K-Means clustering has a wide range of applications in various real-world scenarios. It is a versatile and commonly used clustering algorithm that helps solve many different types of problems. Here are some applications of K-Means clustering in real-world scenarios:\n",
    "\n",
    "Image Compression:\n",
    "\n",
    "K-Means clustering is used to reduce the number of colors in an image. By clustering similar colors together and replacing them with the centroid color of each cluster, you can compress images while maintaining visual quality.\n",
    "Customer Segmentation:\n",
    "\n",
    "In marketing and e-commerce, K-Means is used to group customers into segments based on their purchasing behavior, demographics, or other features. This segmentation can help tailor marketing strategies and product recommendations.\n",
    "Anomaly Detection:\n",
    "\n",
    "K-Means can be used to identify outliers or anomalies in data. Data points that are far from any cluster centroid can be considered anomalies or potential fraud cases in financial transactions.\n",
    "Recommendation Systems:\n",
    "\n",
    "K-Means can be used to segment users into groups with similar preferences. These user segments can be the basis for building recommendation systems that suggest products or content based on the preferences of similar users.\n",
    "Text Document Clustering:\n",
    "\n",
    "K-Means is used in natural language processing to cluster text documents into topics or themes. This helps in document organization, topic modeling, and information retrieval.\n",
    "Genomic Data Analysis:\n",
    "\n",
    "K-Means clustering can group genes with similar expression patterns in genomics research. This can help identify genes related to specific biological processes or diseases.\n",
    "Network Traffic Analysis:\n",
    "\n",
    "K-Means is used to analyze network traffic data to identify patterns and detect network intrusions or anomalies.\n",
    "Image Segmentation:\n",
    "\n",
    "In computer vision, K-Means clustering can be applied to segment images into regions with similar colors or textures. This is useful in object recognition and scene understanding.\n",
    "Remote Sensing:\n",
    "\n",
    "K-Means clustering is used to analyze satellite imagery and remote sensing data to classify land cover, monitor environmental changes, and track natural disasters.\n",
    "Retail Inventory Management:\n",
    "\n",
    "K-Means clustering helps retailers manage inventory by clustering products based on sales patterns. This allows for better stock management and demand forecasting.\n",
    "Healthcare:\n",
    "\n",
    "K-Means can be used in healthcare to cluster patients with similar medical profiles, enabling personalized treatment plans and medical research.\n",
    "Manufacturing:\n",
    "\n",
    "In manufacturing, K-Means clustering is used for quality control, identifying defective products, and optimizing production processes.\n",
    "Crime Analysis:\n",
    "\n",
    "K-Means clustering can help police departments analyze crime data to identify high-crime areas and allocate resources more effectively.\n",
    "Environmental Monitoring:\n",
    "\n",
    "K-Means clustering can be applied to analyze environmental sensor data, such as air quality or weather measurements, to identify pollution sources or unusual weather patterns.\n",
    "These are just a few examples of the many applications of K-Means clustering. Its simplicity, efficiency, and ability to uncover hidden patterns in data make it a valuable tool in a wide range of fields and industries. However, it's important to choose the appropriate clustering algorithm and fine-tune its parameters to fit the specific characteristics of the data and the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d34435-acab-49ef-9025-d773604558ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7300943-8000-4929-b792-f6a9db3c34b5",
   "metadata": {},
   "source": [
    "Interpreting the output of a K-Means clustering algorithm is a critical step in understanding the underlying structure of your data. When you run K-Means, you obtain clusters of data points, each assigned to a specific cluster. Here's how to interpret and derive insights from the resulting clusters:\n",
    "\n",
    "Cluster Centers (Centroids):\n",
    "\n",
    "The cluster centers are the average positions of data points within each cluster. These centroids represent the \"typical\" data point in each cluster.\n",
    "Cluster Assignments:\n",
    "\n",
    "Each data point is assigned to one of the clusters based on its proximity to the cluster's centroid. This assignment tells you which cluster a data point belongs to.\n",
    "Visual Inspection:\n",
    "\n",
    "Visualizing the data points and cluster assignments can provide a quick understanding of the results. Use scatter plots or other visualizations to see how data points are grouped.\n",
    "Cluster Sizes:\n",
    "\n",
    "You can analyze the number of data points in each cluster. Understanding the size of each cluster can provide insights into the relative importance or prevalence of different groups in your data.\n",
    "Cluster Characteristics:\n",
    "\n",
    "Examine the characteristics of data points within each cluster, such as their means, variances, or other relevant statistics. This can help you understand the common traits of the data points in each group.\n",
    "Comparison Across Clusters:\n",
    "\n",
    "Compare the clusters with each other to identify differences and similarities. What distinguishes one cluster from another? Are there clusters with similar characteristics?\n",
    "Validation Metrics:\n",
    "\n",
    "Use clustering validation metrics like the Silhouette Score or Davies-Bouldin Index to assess the quality of the clustering. Higher silhouette scores indicate well-separated clusters, while lower Davies-Bouldin values imply better clustering.\n",
    "Domain Knowledge:\n",
    "\n",
    "Incorporate domain knowledge and expertise to interpret the clusters. Subject matter experts can provide context and insights into what the clusters represent and their practical implications.\n",
    "Hypothesis Testing:\n",
    "\n",
    "Formulate hypotheses about the differences between clusters and test them statistically. This can help confirm or refute assumptions and findings.\n",
    "Use Case-Specific Insights:\n",
    "\n",
    "Consider the goals of your analysis and what insights are most relevant to your use case. For example, in customer segmentation, you might look at purchasing behavior, demographics, or lifetime value to derive actionable insights.\n",
    "Cluster Profiling:\n",
    "\n",
    "Create profiles or descriptions of each cluster. These profiles can include statistical summaries and visualizations to highlight key characteristics and differences.\n",
    "Actionable Recommendations:\n",
    "\n",
    "Once you've gained insights into the clusters, consider what actions or decisions can be made based on the cluster assignments. This may involve tailoring marketing strategies, optimizing resource allocation, or making personalized recommendations.\n",
    "Iterative Analysis:\n",
    "\n",
    "It's often beneficial to perform iterative analysis by adjusting parameters, re-running the algorithm, or combining K-Means with other techniques to refine the results and uncover deeper insights.\n",
    "Interpreting K-Means results can be an iterative process that combines quantitative analysis, domain expertise, and visual exploration. The goal is to understand the underlying structure of your data and use that understanding to make informed decisions or take actions that benefit your specific application or problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba79a8-6bc1-4de2-96a2-7cf4450d0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00718e55-9492-4841-bf0d-ebebb1349e3e",
   "metadata": {},
   "source": [
    "Implementing K-Means clustering can present several challenges, and it's important to be aware of these challenges and know how to address them. Here are some common challenges and potential solutions:\n",
    "\n",
    "Choosing the Optimal K:\n",
    "\n",
    "Challenge: Selecting the right number of clusters (K) is often subjective and can impact the quality of clustering.\n",
    "Solution: Use methods like the Elbow Method, Silhouette Score, Gap Statistics, or cross-validation to help determine the optimal K. Additionally, consider domain knowledge and problem-specific insights.\n",
    "Sensitive to Initialization:\n",
    "\n",
    "Challenge: K-Means is sensitive to the initial placement of cluster centroids, leading to different results with different initializations.\n",
    "Solution: Run the algorithm multiple times with different initializations and choose the result with the lowest inertia or the highest Silhouette Score. This helps mitigate the impact of random initialization.\n",
    "Non-Globular Clusters:\n",
    "\n",
    "Challenge: K-Means assumes that clusters are spherical and of similar size, making it less effective for non-globular or unevenly sized clusters.\n",
    "Solution: Consider using other clustering algorithms like DBSCAN or spectral clustering, which are better suited for handling non-globular clusters.\n",
    "Scaling and Preprocessing:\n",
    "\n",
    "Challenge: K-Means is sensitive to the scale and units of the features, and this can affect the results.\n",
    "Solution: Standardize or normalize the features before applying K-Means to ensure that features with different units contribute equally to the clustering. Preprocessing can also help improve the algorithm's performance.\n",
    "Outliers:\n",
    "\n",
    "Challenge: Outliers can significantly impact cluster centroids and assignments, leading to suboptimal results.\n",
    "Solution: Consider outlier detection techniques to identify and handle outliers before running K-Means. Alternatively, use robust K-Means variants that are less sensitive to outliers.\n",
    "Empty Clusters:\n",
    "\n",
    "Challenge: K-Means may produce empty clusters, especially when using a small value of K or with irregularly distributed data.\n",
    "Solution: Avoid very small values of K, use hierarchical clustering to obtain an initial hierarchy, or choose other clustering algorithms that are less prone to empty clusters.\n",
    "Interpretability:\n",
    "\n",
    "Challenge: Interpreting the meaning and relevance of clusters can be challenging, especially when dealing with high-dimensional data.\n",
    "Solution: Use visualization, cluster profiling, and domain expertise to gain insights into the clusters. Consider dimensionality reduction techniques like PCA to reduce the dimensionality while preserving important information.\n",
    "Computational Complexity:\n",
    "\n",
    "Challenge: K-Means can be computationally expensive for large datasets or high-dimensional data.\n",
    "Solution: Use mini-batch K-Means for large datasets, or apply dimensionality reduction techniques to reduce the data's dimensionality.\n",
    "Choosing Distance Metric:\n",
    "\n",
    "Challenge: The choice of distance metric can affect the clustering results, and it may not always be clear which metric is the most appropriate.\n",
    "Solution: Experiment with different distance metrics based on the nature of your data and the problem. Choose a metric that aligns with the characteristics of your data and your clustering goals.\n",
    "Overfitting:\n",
    "\n",
    "Challenge: K-Means can overfit to noise or irrelevant features if the number of clusters is too large.\n",
    "Solution: Be cautious about choosing an excessively high value of K. Use validation metrics and visual inspection to assess the quality of the clusters and avoid overfitting.\n",
    "Addressing these challenges requires a combination of data preprocessing, parameter tuning, and a good understanding of the data and problem context. It's also important to be open to exploring other clustering algorithms if K-Means proves to be unsuitable for your specific data and goals.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f222ea4-6a6f-4f8e-a08f-4324c7067bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c913f-37ad-4c18-bf49-b4fd7c51d393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f34a77-1bad-4edd-8df1-13109d257f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2b101-d0cb-42ce-9b4f-d599cd054f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c18e7-096f-470a-9082-adb32d73fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6a440-af14-46ef-ac59-1aff87934d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e659183-7426-450a-a401-983e5983bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c80f33-7ffe-4574-a653-648585efca9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf769d-fe89-4f79-9421-30094eb78b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
