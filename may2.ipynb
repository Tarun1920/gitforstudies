{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa36c5-6c03-4913-9a4c-ca112669add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d919c-b1ee-41ca-a4a5-04a1aef10391",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis and machine learning to identify patterns or observations in a dataset that deviate significantly from the norm or expected behavior. The purpose of anomaly detection is to highlight unusual or rare events that may indicate errors, outliers, or potential issues in a system. These anomalies could represent interesting and valuable insights, such as fraud detection in financial transactions, equipment failures in manufacturing, or unusual patterns in network traffic indicating a security breach.\n",
    "\n",
    "The process of anomaly detection typically involves establishing a baseline of normal behavior from historical data and then identifying instances that deviate significantly from this baseline. There are various methods and algorithms used for anomaly detection, including statistical methods, machine learning approaches, and unsupervised learning techniques.\n",
    "\n",
    "Anomaly detection is widely used in diverse fields, including finance, cybersecurity, healthcare, manufacturing, and more, to improve decision-making processes by flagging unusual events that may require further investigation or intervention.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8994e7-54c6-4db1-8b3d-7e1fe8d6d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22048878-d916-4302-8213-e72adeee102a",
   "metadata": {},
   "source": [
    "Anomaly detection comes with its own set of challenges, and addressing these challenges is crucial for the successful implementation of anomaly detection systems. Some key challenges include:\n",
    "\n",
    "Unlabeled Data: In many real-world scenarios, labeled data (instances explicitly marked as normal or anomalous) for training machine learning models is scarce. Anomaly detection often relies on unsupervised learning techniques, making it challenging to accurately define what constitutes a normal pattern.\n",
    "\n",
    "Class Imbalance: Anomalies are typically rare events compared to normal instances, leading to class imbalance in the dataset. Imbalanced datasets can affect the performance of machine learning models, as they may be biased toward the majority class.\n",
    "\n",
    "Dynamic Environments: The nature of data in many applications can change over time. Anomaly detection systems need to adapt to these changes and continuously update their models to maintain effectiveness.\n",
    "\n",
    "Feature Engineering: Choosing relevant features is critical for the success of anomaly detection. Identifying which features are most indicative of normal behavior and anomalies can be a complex task, and selecting inappropriate features may lead to poor performance.\n",
    "\n",
    "Noise and Outliers: Noise and outliers in the data can be confused with anomalies. Anomaly detection algorithms need to be robust to noise and capable of distinguishing between truly anomalous patterns and data artifacts.\n",
    "\n",
    "Scalability: In large-scale systems, processing and analyzing vast amounts of data in real-time can be challenging. Anomaly detection algorithms must be scalable to handle the volume and velocity of data generated by modern applications.\n",
    "\n",
    "Interpretability: Understanding why a particular instance is flagged as anomalous is crucial, especially in applications where human intervention is required. Black-box models may lack interpretability, making it difficult to trust and act upon their decisions.\n",
    "\n",
    "Adversarial Attacks: In some applications, malicious entities may intentionally try to manipulate the system by generating anomalous patterns that mimic normal behavior. Anomaly detection models should be designed to be resilient to such adversarial attacks.\n",
    "\n",
    "Addressing these challenges often involves a combination of domain expertise, careful feature selection, appropriate algorithm choices, and ongoing monitoring and adaptation of the anomaly detection system.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e7e17-6759-40a4-9058-9a6943c00161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfc9ef-59be-40e3-892c-4269a2cd5d19",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches used in the field of anomaly detection, and they differ in terms of the availability of labeled training data and the way models are trained.\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "\n",
    "Training Data: Unsupervised anomaly detection operates on datasets where instances are not labeled as normal or anomalous. The algorithm works without prior knowledge of what constitutes an anomaly.\n",
    "Algorithm Operation: These methods aim to learn the inherent patterns and structures within the data and identify instances that deviate significantly from these learned patterns. Common techniques include clustering, density estimation, and distance-based methods.\n",
    "Applicability: Unsupervised methods are particularly useful when labeled anomaly examples are scarce or when the nature of anomalies is not well-defined in advance. They are more flexible but may have difficulty distinguishing between different types of anomalies.\n",
    "Supervised Anomaly Detection:\n",
    "\n",
    "Training Data: In supervised anomaly detection, the algorithm is trained on a dataset where instances are labeled as either normal or anomalous. The model learns from examples of both normal and anomalous behavior during training.\n",
    "Algorithm Operation: Supervised methods use the labeled data to learn the characteristics that differentiate normal instances from anomalies. This knowledge is then used to classify new, unseen instances as either normal or anomalous during the testing or deployment phase.\n",
    "Applicability: Supervised methods are effective when a sufficient amount of labeled data is available and when the types of anomalies are well-defined. They can be more precise in identifying anomalies but may struggle when faced with novel or previously unseen types of anomalies.\n",
    "In summary, the main difference lies in the availability of labeled training data. Unsupervised anomaly detection is more exploratory and does not rely on labeled anomalies during training, making it suitable for situations where labeled data is scarce. Supervised anomaly detection, on the other hand, leverages labeled data to train models that can make more targeted decisions but requires a substantial amount of labeled examples for both normal and anomalous instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d2e36-b126-450d-8124-5644feb1d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788fcd6-2ab0-4879-bcd4-54773e88ef06",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several main types, each with its own approach to identifying anomalies. The main categories include:\n",
    "\n",
    "Statistical Methods:\n",
    "\n",
    "Z-Score: This method involves calculating the standard score (Z-score) of each data point to identify deviations from the mean.\n",
    "Quartile or Percentile Ranges: Outliers can be identified by considering data points outside certain quartile or percentile ranges.\n",
    "Machine Learning-Based Methods:\n",
    "\n",
    "Clustering Algorithms: Unsupervised clustering algorithms, such as k-means or hierarchical clustering, can identify anomalies by isolating data points that do not belong to any cluster or are in small clusters.\n",
    "One-Class SVM (Support Vector Machines): This algorithm learns the characteristics of normal instances during training and identifies anomalies as instances lying significantly outside the learned boundary.\n",
    "Isolation Forest: This algorithm isolates anomalies by recursively partitioning the data, making it efficient for high-dimensional datasets.\n",
    "Autoencoders: Neural network-based autoencoders learn a compressed representation of normal data and identify anomalies based on reconstruction errors.\n",
    "Density-Based Methods:\n",
    "\n",
    "Kernel Density Estimation (KDE): KDE estimates the probability density function of the data, and anomalies are identified as points in low-density regions.\n",
    "Local Outlier Factor (LOF): LOF measures the local density deviation of a data point with respect to its neighbors, identifying points with significantly lower density as anomalies.\n",
    "Distance-Based Methods:\n",
    "\n",
    "Mahalanobis Distance: It measures the distance of a data point from the centroid, considering the covariance between features. Anomalies have higher Mahalanobis distances.\n",
    "Euclidean Distance: Anomalies can be identified by considering data points that are farther away from the centroid or mean of the data.\n",
    "Ensemble Methods:\n",
    "\n",
    "Isolation Forest Ensembles: Combining multiple isolation forests can enhance the robustness of anomaly detection.\n",
    "Random Forests: Using an ensemble of decision trees, where anomalies are identified based on their frequency of occurrence in the trees.\n",
    "Proximity-Based Methods:\n",
    "\n",
    "Nearest Neighbor Methods: Anomalies are identified based on the distance or dissimilarity to their nearest neighbors.\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): This clustering algorithm identifies anomalies as points that do not belong to any dense cluster.\n",
    "These categories are not mutually exclusive, and hybrid approaches often combine elements from multiple categories to improve overall performance. The choice of an anomaly detection algorithm depends on the characteristics of the data and the specific requirements of the application.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ae533-a0d1-4026-acbd-9baab439485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e6b3-c773-40b4-9352-d903718854df",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make certain assumptions about the distribution of normal and anomalous data points in the feature space. These assumptions influence the effectiveness of these methods and their ability to accurately identify anomalies. The main assumptions include:\n",
    "\n",
    "Normal Instances Form Clusters:\n",
    "\n",
    "Assumption: Distance-based methods often assume that normal instances tend to cluster together in the feature space.\n",
    "Rationale: In a typical dataset, normal instances are expected to exhibit similar patterns and behaviors, leading to the formation of clusters. Anomalies are then considered as instances that fall outside these clusters.\n",
    "Anomalies Are Isolated:\n",
    "\n",
    "Assumption: Anomalous instances are assumed to be relatively isolated or far away from the dense regions of normal instances.\n",
    "Rationale: The premise is that anomalies represent deviations from the typical patterns observed in normal data. Therefore, they are expected to have larger distances or dissimilarities to their nearest neighbors in comparison to normal instances.\n",
    "Homogeneous Density of Normal Instances:\n",
    "\n",
    "Assumption: The density of normal instances is assumed to be relatively homogeneous across the feature space.\n",
    "Rationale: Distance-based methods often assume that normal instances are distributed uniformly or smoothly in the feature space. This assumption supports the idea that anomalies, being rare and different, will have higher distances to their neighbors.\n",
    "Euclidean Distance as a Measure:\n",
    "\n",
    "Assumption: Many distance-based methods, especially those using Euclidean distance, assume that the distance metric adequately captures the dissimilarity between data points.\n",
    "Rationale: Euclidean distance is a commonly used metric in distance-based anomaly detection. It assumes that features are numeric and continuous, and the geometric distance between points accurately reflects their dissimilarity.\n",
    "Single Density Region for Normal Instances:\n",
    "\n",
    "Assumption: Normal instances are often assumed to belong to a single, connected density region.\n",
    "Rationale: The assumption is that normal instances share common characteristics and form a cohesive cluster. Anomalies, deviating from these shared characteristics, are expected to be detected as points outside this primary density region.\n",
    "It's important to note that the effectiveness of distance-based anomaly detection methods can be influenced by the degree to which these assumptions hold true in a given dataset. If the data does not conform to these assumptions, alternative anomaly detection methods, such as clustering-based or density-based methods, may be more appropriate. Additionally, careful consideration of the specific characteristics of the data is essential when choosing and interpreting the results of distance-based anomaly detection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db283c94-4aa4-40cd-8b27-41822343a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308197f-29a8-4981-a803-14ade33b0152",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a density-based anomaly detection method that measures the local density deviation of a data point with respect to its neighbors. The LOF algorithm assigns an anomaly score to each data point, indicating its degree of \"outlierness\" compared to its local neighborhood. Here's a brief overview of how LOF computes anomaly scores:\n",
    "\n",
    "Local Reachability Density (LRD):\n",
    "\n",
    "For each data point \n",
    "�\n",
    "p, the algorithm calculates the reachability distance (\n",
    "reachdist\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "reachdist \n",
    "k\n",
    "​\n",
    " (p,o)) to its \n",
    "�\n",
    "k-th nearest neighbor \n",
    "�\n",
    "o.\n",
    "The reachability distance is the maximum of the Euclidean distance between \n",
    "�\n",
    "p and \n",
    "�\n",
    "o and the \n",
    "�\n",
    "k-distance of \n",
    "�\n",
    "o. The \n",
    "�\n",
    "k-distance of a point is the distance to its \n",
    "�\n",
    "k-th nearest neighbor.\n",
    "Mathematically, \n",
    "reachdist\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "dist\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    ",\n",
    "k-distance\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "reachdist \n",
    "k\n",
    "​\n",
    " (p,o)=max(dist(p,o),k-distance \n",
    "k\n",
    "​\n",
    " (o)).\n",
    "Local Reachability Density (LRD) Calculation:\n",
    "\n",
    "The local reachability density (\n",
    "LRD\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "LRD \n",
    "k\n",
    "​\n",
    " (p)) of a data point \n",
    "�\n",
    "p is the inverse of the average reachability distance from \n",
    "�\n",
    "p to its \n",
    "�\n",
    "k-th nearest neighbors. It is calculated as follows:\n",
    "LRD\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "(\n",
    "1\n",
    "avg\n",
    "(\n",
    "reachdist\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    "1\n",
    ")\n",
    ",\n",
    "…\n",
    ",\n",
    "reachdist\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "LRD \n",
    "k\n",
    "​\n",
    " (p)=( \n",
    "avg(reachdist \n",
    "k\n",
    "​\n",
    " (p,o \n",
    "1\n",
    "​\n",
    " ),…,reachdist \n",
    "k\n",
    "​\n",
    " (p,o \n",
    "k\n",
    "​\n",
    " ))\n",
    "1\n",
    "​\n",
    " )\n",
    "Local Outlier Factor (LOF) Calculation:\n",
    "\n",
    "The Local Outlier Factor (\n",
    "LOF\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "LOF \n",
    "k\n",
    "​\n",
    " (p)) for a data point \n",
    "�\n",
    "p is the average ratio of its local reachability density to the local reachability densities of its neighbors. It is calculated as follows:\n",
    "LOF\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "∑\n",
    "�\n",
    "∈\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "LRD\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "LRD\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "LOF \n",
    "k\n",
    "​\n",
    " (p)= \n",
    "k\n",
    "∑ \n",
    "o∈N \n",
    "k\n",
    "​\n",
    " (p)\n",
    "​\n",
    "  \n",
    "LRD \n",
    "k\n",
    "​\n",
    " (p)\n",
    "LRD \n",
    "k\n",
    "​\n",
    " (o)\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "N \n",
    "k\n",
    "​\n",
    " (p) is the set of \n",
    "�\n",
    "k-nearest neighbors of \n",
    "�\n",
    "p.\n",
    "Anomaly Score:\n",
    "\n",
    "The anomaly score for each data point is the average LOF value across different values of \n",
    "�\n",
    "k. A higher LOF indicates that the point is more likely to be an anomaly.\n",
    "In summary, LOF assigns anomaly scores by comparing the local reachability density of a data point to the local reachability densities of its neighbors. Points with lower local reachability densities than their neighbors are likely to have higher LOF values and are considered more anomalous. The algorithm is effective in identifying anomalies that have different local densities compared to their surroundings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9edc4-cf43-49df-8697-e59f8b39d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdcf90-973e-4b14-abae-47171eb9887e",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised machine learning algorithm for anomaly detection that works by isolating anomalies rather than profiling normal instances. It is based on the idea that anomalies are easier to isolate in the feature space than normal instances. The Isolation Forest algorithm has two key parameters:\n",
    "\n",
    "Number of Trees (\n",
    "�\n",
    "_\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "n_trees):\n",
    "\n",
    "This parameter determines the number of isolation trees to build.\n",
    "A higher number of trees can lead to better anomaly detection performance but may also increase the computational cost.\n",
    "Commonly, values in the range of 50 to 1000 trees are used, depending on the size and complexity of the dataset.\n",
    "Subsample Size (\n",
    "�\n",
    "�\n",
    "�\n",
    "_\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "max_samples):\n",
    "\n",
    "This parameter determines the number of samples used to build each isolation tree.\n",
    "A smaller subsample size can improve the algorithm's ability to isolate anomalies by creating more diverse trees.\n",
    "Common values for \n",
    "�\n",
    "�\n",
    "�\n",
    "_\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "max_samples are often set to the size of the training dataset or a fraction of it (e.g., 256, 512, or 0.8 for 80% of the dataset).\n",
    "These parameters allow you to control the trade-off between computational efficiency and the algorithm's ability to detect anomalies accurately. Tuning these parameters involves finding a balance that works well for the specific characteristics of the dataset.\n",
    "\n",
    "In addition to these two key parameters, there are other internal parameters used in the Isolation Forest algorithm, but they are typically set to default values and are not exposed for user tuning. These internal parameters include the maximum depth of each tree and the decision threshold for isolating anomalies. The algorithm is designed to be simple to use, and the default settings often work well for a wide range of datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02448616-06c0-46d5-8e21-9ad82ad213cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cd782-bd29-49b8-a084-f358d094cd60",
   "metadata": {},
   "source": [
    "In k-Nearest Neighbors (KNN) anomaly detection, the anomaly score for a data point is often based on the distance or dissimilarity to its \n",
    "�\n",
    "k-th nearest neighbor. In this case, \n",
    "�\n",
    "=\n",
    "10\n",
    "k=10. If a data point has only 2 neighbors of the same class within a radius of 0.5, it implies that it does not have enough neighbors to satisfy the \n",
    "�\n",
    "k requirement.\n",
    "\n",
    "In typical KNN-based anomaly detection, the algorithm would look for the \n",
    "�\n",
    "k-th nearest neighbor, and if there are not enough neighbors, it might not provide a meaningful anomaly score. However, for the sake of explanation, let's consider a scenario where we want to calculate an anomaly score based on the available neighbors:\n",
    "\n",
    "Available Neighbors: 2 neighbors within a radius of 0.5.\n",
    "Required \n",
    "�\n",
    "k: 10.\n",
    "In this case, you might not have enough neighbors to calculate a meaningful anomaly score based on the traditional \n",
    "�\n",
    "k-th nearest neighbor approach. The anomaly score calculation often involves considering the distances to the \n",
    "�\n",
    "k-th nearest neighbor, and if \n",
    "�\n",
    "k neighbors are not available, the algorithm may not provide a reliable score.\n",
    "\n",
    "It's important to note that in practice, if you are using KNN for anomaly detection, you would typically choose a value of \n",
    "�\n",
    "k such that it provides a reasonable number of neighbors for the density estimation. Having too few neighbors can lead to overfitting and unreliable anomaly scores. If you have only 2 neighbors within a small radius, it might be beneficial to reconsider the choice of \n",
    "�\n",
    "k or the distance threshold for defining neighbors.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695b90c-13d3-41d2-8431-84235fca46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c44b01-3ed5-45e9-95fd-f1a5183b681f",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is determined by the average path length across all isolation trees. The average path length is a measure of how quickly a point is isolated within a tree. Smaller average path lengths indicate that a point is isolated more quickly and is considered more likely to be an anomaly.\n",
    "\n",
    "The formula for the anomaly score (\n",
    "�\n",
    "S) of a data point with average path length (\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "E(h(x))) compared to the average path length of the trees (\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "ˉ\n",
    "E(h(x))\n",
    "ˉ\n",
    "​\n",
    " ) is given by:\n",
    "\n",
    "�\n",
    "=\n",
    "2\n",
    "−\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "ˉ\n",
    "S=2 \n",
    "− \n",
    "E(h(x))\n",
    "ˉ\n",
    "​\n",
    " \n",
    "E(h(x))\n",
    "​\n",
    " \n",
    " \n",
    "\n",
    "In your case:\n",
    "\n",
    "Number of trees (\n",
    "�\n",
    "_\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "n_trees): 100\n",
    "Dataset size: 3000 data points\n",
    "Average path length for the data point (\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "E(h(x))): 5.0\n",
    "Average path length across all trees (\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "ˉ\n",
    "E(h(x))\n",
    "ˉ\n",
    "​\n",
    " ): To be provided or calculated based on the training data\n",
    "You need to know the value of \n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "ˉ\n",
    "E(h(x))\n",
    "ˉ\n",
    "​\n",
    "  to compute the anomaly score. If you have the average path length across all trees, you can substitute these values into the formula to calculate the anomaly score.\n",
    "\n",
    "If the average path length for the trees is, for example, \n",
    "�\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "ˉ\n",
    "=\n",
    "10.0\n",
    "E(h(x))\n",
    "ˉ\n",
    "​\n",
    " =10.0, then the anomaly score (\n",
    "�\n",
    "S) would be:\n",
    "\n",
    "�\n",
    "=\n",
    "2\n",
    "−\n",
    "5.0\n",
    "10.0\n",
    "=\n",
    "0.707\n",
    "S=2 \n",
    "− \n",
    "10.0\n",
    "5.0\n",
    "​\n",
    " \n",
    " =0.707\n",
    "\n",
    "The resulting anomaly score ranges between 0 and 1, with lower scores indicating a higher likelihood of the data point being an anomaly. The specific interpretation of the anomaly score threshold depends on the characteristics of the dataset and the application's requirements.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624c5e0-0b14-499a-a2ea-08b101c4d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6259c-3e68-4474-a94b-e907182162c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee7cf9-2e23-422e-a736-78f181a726e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eca67-c01e-45d8-8d04-952ec642a60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b853ce-ec48-4423-99aa-c8ff1b171948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117095f-4d3e-4c4b-a0fd-7024218b5d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb821937-e9e4-439f-8ba5-247376ee3224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0201ce-89de-48a3-9e9e-2fcca7a3a6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
